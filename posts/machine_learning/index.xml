<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine Learning on Anna-Lena Popkes</title><link>https://alpopkes.com/posts/machine_learning/</link><description>Recent content in Machine Learning on Anna-Lena Popkes</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 11 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alpopkes.com/posts/machine_learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Principal component analysis (PCA)</title><link>https://alpopkes.com/posts/machine_learning/principal_component_analysis/</link><pubDate>Thu, 11 Aug 2022 00:00:00 +0000</pubDate><guid>https://alpopkes.com/posts/machine_learning/principal_component_analysis/</guid><description>After a longer break I continued working on my machine learning basics repository which implements fundamental machine learning algorithms in plain Python. This time, I took a detailed look at principal component analysis (PCA). The blog post below contains the same content as the original notebook. You can run the notebook directly in your Browser using Binder.
1. What is PCA? In simple terms, principal component analysis (PCA) is a technique to perform dimensionality reduction.</description></item><item><title>Support vector machines</title><link>https://alpopkes.com/posts/machine_learning/support_vector_machines/</link><pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate><guid>https://alpopkes.com/posts/machine_learning/support_vector_machines/</guid><description>I posted another notebook in my machine learning basics repository. This time, I took a detailed look at support vector machines. The blog post below contains the same content as the original notebook. You can run the notebook directly in your Browser using Binder.
1. What are support vector machines? Support vector machines (short: SVMs) are supervised machine learning models. They are the most prominent member of the class of kernel methods.</description></item><item><title>Bayesian linear regression</title><link>https://alpopkes.com/posts/machine_learning/bayesian_linear_regression/</link><pubDate>Sat, 20 Feb 2021 00:00:00 +0000</pubDate><guid>https://alpopkes.com/posts/machine_learning/bayesian_linear_regression/</guid><description>I finally found time to continue working on my machine learning basics repository which implements fundamental machine learning algorithms in plain Python. Especially, I took a detailed look at Bayesian linear regression. The blog post below contains the same content as the original notebook. You can run the notebook directly in your Browser using Binder.
1. What is Bayesian linear regression (BLR)? Bayesian linear regression is the Bayesian interpretation of linear regression.</description></item><item><title>Variational Inference</title><link>https://alpopkes.com/posts/machine_learning/variational_inference/</link><pubDate>Sat, 23 Feb 2019 00:00:00 +0000</pubDate><guid>https://alpopkes.com/posts/machine_learning/variational_inference/</guid><description>Introduction Variational inference is an important topic that is widely used in machine learning. For example, it&amp;rsquo;s the basis for variational autoencoders. Also Bayesian learning often makes use variational of inference. To understand what variational inference is, how it works and why it&amp;rsquo;s useful we will go through each point step by step.
What are latent variables? A latent variable is the opposite of an observed variable. This means that a latent variable is not directly observed but inferred from other variables which are observed.</description></item><item><title>Kullback-Leibler Divergence</title><link>https://alpopkes.com/posts/machine_learning/kl_divergence/</link><pubDate>Sat, 02 Feb 2019 00:00:00 +0000</pubDate><guid>https://alpopkes.com/posts/machine_learning/kl_divergence/</guid><description>One of the points on my long &amp;lsquo;stuff-you-have-to-look-at&amp;rsquo; list is the Kullback-Leibler divergence. I finally took the time to take a detailed look at this topic.
Definition The KL-divergence is a measure of how similar (or different) two probablity distributions are. When having a discrete probability distribution $P$ and another probability distribution $Q$ the KL-divergence for a set of points $X$ is defined as:
$$D_{KL}(P ,|| ,Q) = \sum_{x \in X} P(x) \log \big( \frac{P(x)}{Q(x)} \big)$$</description></item></channel></rss>