<!doctype html><html><head><title>Support vector machines</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/navigators/navbar.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/images/author/image_al_hu6aa485cfd64a98f54b411e3a0324b396_178285_42x0_resize_q75_box.jpg><link rel=stylesheet href=/css/style.css><meta name=description content="Support vector machines"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/images/author/image_al_hu6aa485cfd64a98f54b411e3a0324b396_178285_42x0_resize_q75_box.jpg>Anna-Lena Popkes</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/images/author/image_al_hu6aa485cfd64a98f54b411e3a0324b396_178285_42x0_resize_q75_box.jpg class=d-none id=main-logo>
<img src=/images/author/image_al_hu6aa485cfd64a98f54b411e3a0324b396_178285_42x0_resize_q75_box.jpg class=d-none id=inverted-logo></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><input type=text placeholder=Search data-search id=search-box><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><a href=/posts/my_path_to_ml/>My path to machine learning</a></li><li><i class="fas fa-plus-circle"></i><a href=/posts/books/>Books</a><ul><li><a href=/posts/books/reading_list/>Personal reading List</a></li><li><a href=/posts/books/deep_work/>Deep work</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/machine_learning/>Machine Learning</a><ul class=active><li><a href=/posts/machine_learning/bayesian_linear_regression/>Bayesian linear regression</a></li><li><a href=/posts/machine_learning/kl_divergence/>KL Divergence</a></li><li><a class=active href=/posts/machine_learning/support_vector_machines/>Support vector machines</a></li><li><a href=/posts/machine_learning/variational_inference/>Variational Inference</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/python/>Python</a><ul><li><a href=/posts/python/mocking/>Mocking</a></li><li><i class="fas fa-plus-circle"></i><a href=/posts/python/magical_universe/>Magical Universe</a><ul><li><a href=/posts/python/magical_universe/day_1_start/>Start</a></li><li><a href=/posts/python/magical_universe/day_1_magical_universe/>The Tales of Castle Kilmere</a></li><li><a href=/posts/python/magical_universe/day_1_first_post_oop/>Object-oriented programming</a></li><li><a href=/posts/python/magical_universe/day_2_types_of_methods/>Types of methods</a></li><li><a href=/posts/python/magical_universe/day_3_type_annotations/>Type annotations</a></li><li><a href=/posts/python/magical_universe/day_4_to_string_conversion/>To-string conversion</a></li><li><a href=/posts/python/magical_universe/day_5_decorators/>Decorators</a></li><li><a href=/posts/python/magical_universe/day_6_properties/>Properties</a></li><li><a href=/posts/python/magical_universe/day_7_underscore_patterns/>Underscore patterns</a></li><li><a href=/posts/python/magical_universe/day_8_extending_universe/>Extending the universe</a></li><li><a href=/posts/python/magical_universe/day_9_duck_typing/>Duck Typing</a></li><li><a href=/posts/python/magical_universe/day_10_11_namedtuples/>Namedtuples</a></li><li><a href=/posts/python/magical_universe/day_12_to_15_abcs/>Abstract Base Classes</a></li><li><a href=/posts/python/magical_universe/day_16_to_18_data_classes/>Data classes</a></li><li><a href=/posts/python/magical_universe/day_19_immutable_data_classes/>Immutable data classes</a></li><li><a href=/posts/python/magical_universe/day_20_decorators_in_classes/>Decorators in classes</a></li><li><a href=/posts/python/magical_universe/day_21_if_main/>if __name__ == "__main__"</a></li><li><a href=/posts/python/magical_universe/day_22_to_24_context_managers/>Context managers</a></li><li><a href=/posts/python/magical_universe/day_25_to_28_pytest/>Testing with pytest</a></li><li><a href=/posts/python/magical_universe/day_29_to_31_iterators/>Iterators</a></li><li><a href=/posts/python/magical_universe/day_34_multisets/>Multisets</a></li><li><a href=/posts/python/magical_universe/day_37_extending_universe/>Extending the universe II</a></li><li><a href=/posts/python/magical_universe/day_43_to_45_exception_classes/>Exception classes</a></li><li><a href=/posts/python/magical_universe/day_46_functools_wraps/>functools.wraps</a></li><li><a href=/posts/python/magical_universe/day_47_to_48_defaultdict/>Defaultdict</a></li><li><a href=/posts/python/magical_universe/day_49_to_50_config_files/>Config files</a></li><li><a href=/posts/python/magical_universe/2018-09-16-blog-post-day-51/>Wrap up</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/software_engineering/>Software Engineering</a><ul><li><a href=/posts/software_engineering/containers/>Intro to containers</a></li><li><a href=/posts/software_engineering/docker/>Intro to Docker</a></li><li><a href=/posts/software_engineering/virtual_machines/>Intro to virtual machines</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://alpopkes.com/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/image_al.jpg><h5 class=author-name>Anna-Lena Popkes</h5><p>April 13, 2021</p></div><div class=title><h1>Support vector machines</h1></div><div class=post-content id=post-content><p>I posted another notebook in my <a href=https://github.com/zotroneneis/machine_learning_basics>machine learning basics repository</a>. This time, I took a detailed look at support vector machines. The blog post below contains the same content as the <a href=https://github.com/zotroneneis/machine_learning_basics/blob/master/support_vector_machines.ipynb>original notebook</a>. You can <a href="https://mybinder.org/v2/gh/zotroneneis/machine_learning_basics/HEAD?filepath=support_vector_machines.ipynb">run the notebook directly in your Browser using Binder</a>.</p><h2 id=1-what-are-support-vector-machines>1. What are support vector machines?</h2><p>Support vector machines (short: SVMs) are supervised machine learning models. They are the most prominent member of the class of <a href=https://en.wikipedia.org/wiki/Kernel_method><em>kernel methods</em></a>. SVMs can be used both for classification and regression. The original SVM proposed in 1963 is a simple binary linear classifier. What does this mean?</p><p>Assume we are given a dataset $D = \big \{ \mathbf{x}_n, y_n \big \}_{n=1}^N$, where $\mathbf{x} \in \mathbb{R}^D$ and labels $y_n \in {-1, +1 }$. A linear (hard-margin) SVM separates the two classes using a ($D-1$ dimensional) hyperplane.</p><p>Special to SVMs is that they use not any hyperplane but the one that maximizes the distance between itself and the two sets of datapoints. Such a hyperplane is called <em>maximum-margin</em> hyperplane:</p><img src=/posts/machine_learning/images/separating_hyperplanes.png width=60% class=center><p>In case you have never heard the term margin: the margin describes the distance between the hyperplane and the closest examples in the dataset.</p><p>Two types of SVMs exist: primals SVMs and dual SVMs. Although most research in the past looked into dual SVMs both can be used to perform non-linear classification. Therefore, we will look at both approaches and compare them in the end.</p><h2 id=2-primal-approach---hard-margin-svm>2. Primal approach - Hard-margin SVM</h2><p>When training an SVM our goal is to find the hyperplane that maximizes the margin between the two sets of points. This hyperplane is fully defined by the points closest to the margin, which are also called <em>support vectors</em>.</p><p>The equation of a hyperplane is given by $\langle \mathbf{w}, \mathbf{x} \rangle + b = 0$. If an example $\mathbf{x}_i$ lies on the right side of the hyperplane (that is, it has a positive label) we have $\langle \mathbf{w}, \mathbf{x} \rangle + b \gt 0$. If instead $\mathbf{x}_i$ lies on the left side (= negative label) we have $\langle \mathbf{w}, \mathbf{x} \rangle + b \lt 0$.</p><p>The support vectors lie exactly on the margin and the optimal separating hyperplane should have the same distance from all support vectors. In this sense the maximum margin hyperplane lies between two separating hyperplanes that are determined by the support vectors:</p><img src=/posts/machine_learning/images/delimiting_hyperplanes.png width=40% class=center><h3 id=21-goal-1>2.1 Goal 1</h3><p>When deriving a formal equation for the maximum margin hyperplane we assume that the two delimiting hyperplanes are given by:<br>$$\langle \mathbf{w}, \mathbf{x}_{+} \rangle + b = +1$$
$$\langle \mathbf{w}, \mathbf{x}_{-} \rangle + b = -1$$</p><p>In other words: we want our datapoints two lie at least a distance of 1 away from the decision hyperplane into both directions. To be more precise: for our positive examples (those with label $y_n = +1$) we want the following to hold: $\langle \mathbf{w}, \mathbf{x}_n \rangle + b \ge +1$.</p><p>For our negative examples (those with label $y_n = -1$) we want the opposite: $\langle \mathbf{w}, \mathbf{x}_n \rangle + b \le -1$. This can be combined into a single equation: $y_n(\langle \mathbf{w}, \mathbf{x}_n \rangle + b) \ge 1$. This is our first goal: <strong>We want a decision boundary that classifies our training examples correctly.</strong></p><h3 id=22-goal-2>2.2 Goal 2</h3><p>Our second goal is to maximize the margin of this decision boundary. The margin is given by $\frac{1}{\mathbf{w}}$. If you would like to understand where this value is coming from take a look at the section &ldquo;<em>(Optional) Deriving the margin equation</em>&rdquo; below.</p><p>Our goal to maximize the margin can be expressed as follows:
$$ \max_{\mathbf{w}, b} \frac{1}{\Vert \mathbf{w} \Vert}$$</p><p>Instead of maximizing $\frac{1}{\Vert \mathbf{w} \Vert}$ we can instead minimize $\frac{1}{2} \Vert \mathbf{w} \Vert^2$. This simplifies the computation of the gradient.</p><h3 id=23-combined-goal>2.3 Combined goal</h3><p>Combining goal one and goal two yields the following objective function:<br>$$
\min_{\mathbf{w}, b} \frac{1}{2} \Vert \mathbf{w} \Vert^2
$$
$$
\text{subject to: } y_n(\langle \mathbf{w}, \mathbf{x}_n \rangle + b) \ge 1 \text{ for all } n = 1, &mldr;, N
$$</p><p>In words: we want to find the values for $\mathbf{w}$ and $b$ that maximize the margin while classifying all training examples correctly. This approach is called the <em>hard-margin support vector machine</em>. &ldquo;Hard&rdquo; because it does not allow for violations of the margin requirement (= no points are allowed to be within the margin).</p><h3 id=24-optional-deriving-the-margin-equation>2.4 (Optional) Deriving the margin equation</h3><p>We can derive the width of the margin in several ways (see sections 12.2.1-12.2.2 of the <a href=https://mml-book.com>Mathematics for Machine Learning book</a>). Personally, I found the explanation of <a href="https://www.youtube.com/watch?v=_PwhiWxHK8o">this MIT lecture on SVMs</a> easiest to understand.</p><p>The derivation of the margin is based on the assumptions that we have already noted above:
$$\langle \mathbf{w}, \mathbf{x}_{+} \rangle + b = +1$$
$$\langle \mathbf{w}, \mathbf{x}_{-} \rangle + b = -1$$</p><p>Including the label of each example we can rewrite this as
$$y_i (\langle \mathbf{w}, \mathbf{x}_{i} \rangle + b) -1 = 0$$</p><p>Let&rsquo;s say we have a positive example $\mathbf{x}_{+}$ that lies on the right delimiting hyperplane and a negative example $\mathbf{x}_{-}$ that lies on the left delimiting hyperplane. The distance between these two vectors is given by ($\mathbf{x}_{+} - \mathbf{x}_{-})$. We want to compute the orthogonal projection of the vector onto the line that is perpendicular to the decision hyperplane. This would give us the width between the two delimiting hyperplanes. We can compute this by multiplying the vector ($\mathbf{x}_{+} - \mathbf{x}_{-})$ with a vector that is perpendicular to the hyperplane. We know that the vector $\mathbf{w}$ is perpendicular to the decision hyperplane. So we can compute the margin by multiplying $(\mathbf{x}_{+} - \mathbf{x}_{-})$ with the vector $\mathbf{w}$ where the latter is divided by the scale $||\mathbf{w}||$ to make it a unit vector.</p><img src=/posts/machine_learning/images/maximum_margin_derivation.png width=40% class=center><p>$$
\begin{align}
\text{width} &= (\mathbf{x}_{+} - \mathbf{x}_{-}) \cdot \frac{\mathbf{w}}{||\mathbf{w}||} \\\<br>&= \frac{\mathbf{x}_{+} \cdot \mathbf{w}}{||\mathbf{w}||} - \frac{\mathbf{x}_{-} \cdot \mathbf{w}}{||\mathbf{w}||}
\end{align}
$$</p><p>For the positive example $\mathbf{x}_{+}$ we have $y_+ = +1$ and therefore $(\langle \mathbf{w}, \mathbf{x}_{+} \rangle = 1 - b$. For the negative example $\mathbf{x}_{-}$ we have $y_- = -1$ and therefore $- (\langle \mathbf{w}, \mathbf{x}_{-} \rangle) = 1 + b$:</p><p>$$
\begin{align}
\text{width} &= (\mathbf{x}_{+} - \mathbf{x}_{-}) \cdot \frac{\mathbf{w}}{||\mathbf{w}||} \\\<br>&= \frac{\mathbf{x}_{+} \cdot \mathbf{w}}{||\mathbf{w}||} - \frac{\mathbf{x}_{-} \cdot \mathbf{w}}{||\mathbf{w}||} \\\<br>&= \frac{(1 - b) + (1 + b)}{||\mathbf{w}||}\\\<br>&= \frac{2}{||\mathbf{w}||}\\\<br>\end{align}
$$</p><p>We conclude that the width between the two delimiting hyperplanes equals $\frac{2}{\mathbf{w}}$. And therefore, that the distance between the decision hyperplane and each delimiting hyperplane is $\frac{1}{\mathbf{w}}$.</p><h2 id=3-primal-approach---soft-margin-svm>3. Primal approach - Soft-margin SVM</h2><p>In most real-world situations the available data is not linearly separable. Even if it is, we might prefer a solution which separates the data well while ignoring some noisy examples and outliers. This motivated an extension of the original hard-margin SVM called <em>soft-margin SVM</em>.</p><p>A soft-margin SVM allows for violations of the margin requirement (= classification errors). In other words: not all training examples need to be perfectly classified. They might fall within the margin or even lie on the wrong side of the decision hyperplane. However, such violations are not for free. We pay a cost for each violation, where the value of the cost depends on how far the example is from meeting the margin requirement.</p><p>To implement this we introduce so called <em>slack variables</em> $\xi_n$. Each training example $(\mathbf{x}_n, y_n)$ is assigned a slack variable $\xi_n \ge 0$. The slack variable allows this example to be within the margin or even on the wrong side of the decision hyperplane:</p><ul><li>If $\xi_n = 0$ the training example $(\mathbf{x}_n, y_n)$ lies exactly on the margin</li><li>$0 \lt \xi_n \lt 1$ the training example lies within the margin but on the correct side of the decision hyperplane</li><li>$\xi_n \ge 1$ the training example lies on the wrong side of the decision hyperplane</li></ul><p>We extend our objective function to include the slack variables as follows:
$$ \min_{\mathbf{w}, b, \mathbf{\xi}} \frac{1}{2} \Vert \mathbf{w} \Vert^2 + C \sum_{n=1}^N \xi_n $$</p><p>$$ \text{subject to:} $$</p><p>$$ \begin{equation}
y_n(\langle \mathbf{w}, \mathbf{x}_n \rangle + b) \ge 1 - \xi_n
\end{equation}$$</p><p>$$ \xi_i \gt 0 \text{ for all } n = 1, &mldr;, N $$</p><p>The parameter $C$ is a regularization term that controls the trade-off between maximizing the margin and minimizing the training error (which in turn means classifying all training examples correctly). If the value of $C$ is small, we care more about maximizing the margin than classifying all points correctly. If the value of $C$ is large, we care more about classifying all points correctly than maximizing the margin.</p><h2 id=4-solving-the-primal-optimization-problem>4. Solving the primal optimization problem</h2><p>Theoretically, the primal SVM can be solved in multiple ways. The most well known way is to use the <a href=https://en.wikipedia.org/wiki/Hinge_loss>hinge loss function</a> together with <a href=https://en.wikipedia.org/wiki/Subgradient_method>subgradient descent</a>.</p><h3 id=41-hinge-loss-function>4.1 Hinge loss function</h3><p>The hinge loss function given the true target $y \in {-1, +1}$ and the prediction $f(\mathbf{x}) = \langle\mathbf{w}, \mathbf{x}\rangle+b$ is computed as follows:</p><p>$$\ell(t)=\max {0,1-t} \quad \text{where} \quad t=y \cdot f(\mathbf{x})= y \cdot \big(\langle\mathbf{w}, \mathbf{x}\rangle+b\big)$$</p><p>Let&rsquo;s understand the output of this loss function with a few examples:</p><ul><li>If a training example has label $y = -1$ and the prediction is on the correct side of the marghin (that is, $f(\mathbf{x}) \le -1$), the value of $t$ is larger or equal to $+1$. Therefore, the hinge loss will be zero ($\ell(t) = 0$)</li><li>The same holds if a training example has label $y = 1$ and the prediction is on the correct side of the margin (that is, $f(\mathbf{x}) \ge 1$)</li><li>If a training example ($y = 1$) is on the correct side of the decision hyperplane but lies within the margin (that is, $0 \lt f(\mathbf{x}) \lt 1$) the hinge loss will output a positive value.</li><li>If a training example ($y = 1$) is on the wrong side of the decision hyperplane (that is, $f(\mathbf{x}) \lt 0$), the hinge loss returns an even larger value. This value increases linearly with the distance from the decision hyperplane</li></ul><img src=/posts/machine_learning/images/hinge_loss.png width=50% class=center><h3 id=42-updated-objective-function>4.2 Updated objective function</h3><p>Using the hinge loss we can reformulate the optimization problem of the primal soft-margin SVM. Given a dataset $D = \big \{ \mathbf{x}_n, y_n \big \}_{n=1}^N$ we would like to minimize the total loss which is now given by:</p><p>$$
\min_{\mathbf{w}, b} \frac{1}{2}|\mathbf{w}|^{2} + C \sum_{n=1}^{N} \max \left\{0,1-y_{n}\left(\left\langle\mathbf{w}, \mathbf{x}_{n}\right\rangle+b\right)\right\}
$$</p><p>If you would like to understand why this is equivalent to our previous formulation of the soft-margin SVM please take a look at chapter 12.2.5 of the <a href=https://mml-book.com>Mathematics for Machine Learning book</a>.</p><h3 id=43-optional-three-parts-of-the-objective-function>4.3 [Optional] Three parts of the objective function</h3><p>Our objective function can be divided into three distinct parts:</p><p>Part 1: $\frac{1}{2}|\mathbf{w}|^{2}$</p><p>This part is also called the <em>regularization term</em>. It expresses a preference for solutions that separate the datapoints well, thereby maximizing the margin. In theory, we could replace this term by a different regularization term that expresses a different preference.</p><p>Part 2: $\sum_{n=1}^{N} \max \left\{0,1-y_{n}\left(\left\langle\mathbf{w}, \mathbf{x}_{n}\right\rangle+b\right)\right\}$</p><p>This part is also called the <em>empirical loss</em>. In our case it&rsquo;s the hinge loss which penalizes solutions that make mistakes when classifying the training examples. In theory, this term could be replaced with another loss function that expresses a different preference.</p><p>Part 3: The hyperparameter $C$ that controls the tradeoff between a large margin and a small hinge loss.</p><h3 id=44-sub-gradient-descent>4.4 Sub-gradient descent</h3><p>The hinge loss function is not differentiable (namely at the point $t=1$). Therefore, we cannot compute the gradient right away. However, we can use a method called <a href=https://en.wikipedia.org/wiki/Subgradient_method>subgradient descent</a> to solve our optimization problem. To simplify the derivation we will adapt two things:</p><ol><li>We assume that the bias $b$ is contained in our weight vector as the first entry $w_0$, that is $\mathbf{w} = [b, w_1, &mldr;, w_D]$</li><li>We divide the hinge loss by the number of samples</li></ol><p>Our cost function is then given by
$$
J(\mathbf{w}) = \frac{1}{2}|\mathbf{w}|^{2} + C \frac{1}{N} \sum_{n=1}^{N} \max \left\{0,1-y_{n}\left(\left\langle\mathbf{w}, \mathbf{x}_{n}\right\rangle\right)\right\}
$$</p><p>We will reformulate this to simplify computing the gradient:
$$
J(\mathbf{w}) = \frac{1}{N} \sum_{n=1}^{N} \Big \{ \frac{1}{2}|\mathbf{w}|^{2} + C \max \left\{0,1-y_{n}\left(\left\langle\mathbf{w}, \mathbf{x}_{n}\right\rangle\right)\right\} \Big\}
$$</p><p>The gradient is given by:
$$
\nabla_{w} J(\mathbf{w}) = \frac{1}{N} \sum_{n=1}^N \left\{\begin{array}{ll}
\mathbf{w} & \text{if} \max \left(0,1-y_{n} \left(\langle \mathbf{w}, \mathbf{x}_{n} \rangle \right)\right)=0 \\\<br>\mathbf{w}-C y_{n} \mathbf{x}_{n} & \text { otherwise }
\end{array}\right \}.
$$</p><p>With this formula we can apply stochastic gradient descent to solve the optimization problem.</p><h3 id=45-optional-difference-subgradient-descent-and-gradient-descent>4.5 [Optional] Difference subgradient descent and gradient descent</h3><p>The subgradient method allows us to minimize a non-differentiable convex function. Although looking similar to gradient descent the method has several important differences.</p><h4 id=451-what-is-a-subgradient>4.5.1 What is a subgradient?</h4><p>A subgradient can be described as a generalization of gradients to non-differentiable functions. Informally, a sub-tangent at a point is any line that lies below the function at the point. The subgradient is the slope of this line. Formally, the subgradient a convex function $f$ at $w_0$ is defined as all vectors $g$ such that for any other point $w$</p><p>$$ f(w) - f(w_0) \ge g \cdot (w - w_0) $$</p><p>If $f$ is differentiable at $w_0$, the subgradient contains only one vector which equals the gradient $\nabla f(w_0)$. If, however, $f$ is not differentiable, there may be several values for $g$ that satisfy this inequality. This is illustrated in the figure below.</p><img src=/posts/machine_learning/images/gradient_vs_subgradient.png width=65% class=center><h4 id=452-subgradient-method>4.5.2 Subgradient method</h4><p>To minimize the objective function $f$ the subgradient method uses the following update formula for iteration $k+1$:</p><p>$$ w^{(k+1)} = w^{(k)} - \alpha_k g^{(k)}$$</p><p>Where $g^{(k)}$ is <em>any</em> subgradient of $f$ at $w^{(k)}$ and $\alpha_k$ is the $k$-th step size. Thus, at each iteration, we make a step into the direction of the negative subgradient. When $f$ is differentiable, $g^{(k)}$ equals the gradient $\nabla f(x^{(k)})$ and the method reduces to the standard gradient descent method.</p><p>More details on the subgradient method can be found <a href=https://web.stanford.edu/class/ee392o/subgrad_method.pdf>here</a>.</p><h2 id=5-implementation-primal-approach>5. Implementation primal approach</h2><h3 id=51-toy-dataset>5.1 Toy dataset</h3><p>To implement what we have learned about primal SVMs we first have to generate a dataset. In the cell below we create a simple dataset with two features and labels +1 and -1. We further split the dataset into a test and train set.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> sklearn.datasets <span style=color:#f92672>import</span> make_blobs
<span style=color:#f92672>from</span> sklearn.model_selection <span style=color:#f92672>import</span> train_test_split
<span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#f92672>as</span> plt

features, targets <span style=color:#f92672>=</span> make_blobs(n_samples<span style=color:#f92672>=</span><span style=color:#ae81ff>600</span>, centers<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, n_features<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>)

<span style=color:#75715e># The function outputs targets 0 and 1 so we need to convert targets 0 to -1</span>
transformed_targets <span style=color:#f92672>=</span> [<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span> <span style=color:#66d9ef>if</span> t <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span> <span style=color:#66d9ef>else</span> <span style=color:#f92672>+</span><span style=color:#ae81ff>1</span> <span style=color:#66d9ef>for</span> t <span style=color:#f92672>in</span> targets]
        
<span style=color:#75715e># Plot the dataset</span>
plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>6</span>))
plt<span style=color:#f92672>.</span>scatter(features[:, <span style=color:#ae81ff>0</span>], features[:, <span style=color:#ae81ff>1</span>], c <span style=color:#f92672>=</span> transformed_targets)
plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;Toy dataset&#34;</span>)
plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#34;Feature 2&#34;</span>)
plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#34;Feature 1&#34;</span>)
plt<span style=color:#f92672>.</span>show()

<span style=color:#75715e># Split data into training and test set</span>
features_train, features_test, labels_train, labels_test <span style=color:#f92672>=</span> train_test_split(features, 
                                                                              transformed_targets, 
                                                                              test_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.3</span>)
</code></pre></div><img src=/posts/machine_learning/images/svm_dataset.png width=55% class=center><h3 id=52-svm-class-definition>5.2 SVM class definition</h3><p>Next, we would like to implement an SVM class. We will use the knowledge we already acquired:</p><ol><li>Our objective function using the hinge loss function is given by:
$$
J(\mathbf{w}) = \frac{1}{2}|\mathbf{w}|^{2} + C \frac{1}{N} \sum_{n=1}^{N} \max \left\{0,1-y_{n}\left(\left\langle\mathbf{w}, \mathbf{x}_{n}\right\rangle\right)\right\}
$$</li><li>We can minimize this function by computing the gradient:
$$
\nabla_{w} J(\mathbf{w}) = \frac{1}{N} \sum_{n=1}^N \left \{ \begin{array}{ll}
\mathbf{w} & \text{if} \max \left(0,1-y_{n} \left(\langle \mathbf{w}, \mathbf{x}_{n} \rangle \right)\right)=0 \\\<br>\mathbf{w}-C y_{n} \mathbf{x}_{n} & \text { otherwise }
\end{array} \right \}.
$$</li><li>Given the gradient we use stochastic gradient descent to train our model</li><li>After training our model we can make predictions using the <a href=https://en.wikipedia.org/wiki/Sign_function>sign function</a></li></ol><p>As mentioned previously, we will assume that the bias $b$ is contained in our weight vector as the first entry $w_0$, that is $\mathbf{w} = [b, w_1, &mldr;, w_D] = \mathbf{w} = [w_0, w_1, &mldr;, w_D]$.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> numpy <span style=color:#f92672>as</span> np
<span style=color:#f92672>from</span> sklearn.utils <span style=color:#f92672>import</span> shuffle

<span style=color:#66d9ef>class</span> <span style=color:#a6e22e>LinearSVM</span>:
    
    <span style=color:#66d9ef>def</span> __init__(self, regularization_param):
        <span style=color:#e6db74>&#34;&#34;&#34;
</span><span style=color:#e6db74>        Initialize the model by setting the regularization parameter 
</span><span style=color:#e6db74>        and a boolean variable for our trained weights.
</span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
        self<span style=color:#f92672>.</span>regularization_param <span style=color:#f92672>=</span> regularization_param
        self<span style=color:#f92672>.</span>trained_weights <span style=color:#f92672>=</span> None
    
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>add_bias_term</span>(self, features):
        <span style=color:#e6db74>&#34;&#34;&#34;
</span><span style=color:#e6db74>        Add intercept 1 to each training example for bias b
</span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
        n_samples <span style=color:#f92672>=</span> features<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]
        ones <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>ones((n_samples, <span style=color:#ae81ff>1</span>))
        <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>concatenate((ones, features), axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
    
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_cost</span>(self, weights, features, labels) <span style=color:#f92672>-&gt;</span> float:
        <span style=color:#e6db74>&#34;&#34;&#34;
</span><span style=color:#e6db74>        Compute the value of the cost function
</span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
        n_samples <span style=color:#f92672>=</span> features<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]
        
        <span style=color:#75715e># Compute hinge loss </span>
        predictions <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(features, weights)<span style=color:#f92672>.</span>flatten()
        distances <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> labels <span style=color:#f92672>*</span> predictions
        hinge_losses <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>maximum(<span style=color:#ae81ff>0</span>, distances)

        <span style=color:#75715e># Compute sum of the individual hinge losses</span>
        sum_hinge_loss <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sum(hinge_losses) <span style=color:#f92672>/</span> n_samples

        <span style=color:#75715e># Compute entire cost</span>
        cost <span style=color:#f92672>=</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> <span style=color:#ae81ff>2</span>) <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>dot(weights<span style=color:#f92672>.</span>T, weights) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>regularization_param <span style=color:#f92672>*</span> sum_hinge_loss
        
        <span style=color:#66d9ef>return</span> float(cost)
    
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_gradient</span>(self, weights, features, labels) <span style=color:#f92672>-&gt;</span> np<span style=color:#f92672>.</span>ndarray:
        <span style=color:#e6db74>&#34;&#34;&#34;
</span><span style=color:#e6db74>        Compute the gradient, needed for training
</span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
        predictions <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(features, weights)
        distances <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> labels <span style=color:#f92672>*</span> predictions
        n_samples, n_feat <span style=color:#f92672>=</span> features<span style=color:#f92672>.</span>shape
        sub_gradients <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>1</span>, n_feat))

        <span style=color:#66d9ef>for</span> idx, dist <span style=color:#f92672>in</span> enumerate(distances):
            <span style=color:#66d9ef>if</span> max(<span style=color:#ae81ff>0</span>, dist) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
                sub_gradients <span style=color:#f92672>+=</span> weights<span style=color:#f92672>.</span>T
            <span style=color:#66d9ef>else</span>:
                sub_grad <span style=color:#f92672>=</span> weights<span style=color:#f92672>.</span>T <span style=color:#f92672>-</span> (self<span style=color:#f92672>.</span>regularization_param <span style=color:#f92672>*</span> features[idx] <span style=color:#f92672>*</span> labels[idx])
                sub_gradients <span style=color:#f92672>+=</span> sub_grad
                            
        <span style=color:#75715e># Sum up and divide by the number of samples</span>
        avg_gradient <span style=color:#f92672>=</span> sum(sub_gradients) <span style=color:#f92672>/</span> len(labels)

        <span style=color:#66d9ef>return</span> avg_gradient
    
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train</span>(self, train_features, train_labels, n_epochs, learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>):
        <span style=color:#e6db74>&#34;&#34;&#34;
</span><span style=color:#e6db74>        Train the model with stochastic gradient descent using the
</span><span style=color:#e6db74>        specified number of epochs, learning rate and batch size.
</span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
        <span style=color:#75715e># Add bias term to features</span>
        train_features <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>add_bias_term(train_features)
        
        <span style=color:#75715e># Initalize weight vector</span>
        n_samples, n_feat <span style=color:#f92672>=</span> train_features<span style=color:#f92672>.</span>shape
        weights <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros(n_feat)[:, np<span style=color:#f92672>.</span>newaxis]
        
        <span style=color:#75715e># Train the model for a certain number of epochs</span>
        <span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(n_epochs):
            features, labels <span style=color:#f92672>=</span> shuffle(train_features, train_labels)
            features, labels <span style=color:#f92672>=</span> train_features, train_labels
            start, end <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>, batch_size
            <span style=color:#66d9ef>while</span> end <span style=color:#f92672>&lt;=</span> len(labels): <span style=color:#75715e># Training loop over the dataset</span>
                batch <span style=color:#f92672>=</span> features[start:end]
                batch_labels <span style=color:#f92672>=</span> labels[start:end]
                
                grad <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>compute_gradient(weights, batch, batch_labels)
                update <span style=color:#f92672>=</span> (learning_rate <span style=color:#f92672>*</span> grad)[:, np<span style=color:#f92672>.</span>newaxis]
                weights <span style=color:#f92672>=</span> weights <span style=color:#f92672>-</span> update
                start, end <span style=color:#f92672>=</span> end, end <span style=color:#f92672>+</span> batch_size
                
            current_cost <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>compute_cost(weights, features, labels)
            <span style=color:#66d9ef>print</span>(f<span style=color:#e6db74>&#34;Epoch {epoch + 1}, cost: {current_cost}&#34;</span>)
                
        <span style=color:#75715e># Set the trained weights to allow making predictions</span>
        self<span style=color:#f92672>.</span>trained_weights <span style=color:#f92672>=</span> weights

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict</span>(self, test_features) <span style=color:#f92672>-&gt;</span> np<span style=color:#f92672>.</span>ndarray:
        <span style=color:#e6db74>&#34;&#34;&#34;
</span><span style=color:#e6db74>        Predict labels for new test features.
</span><span style=color:#e6db74>        Raises ValueError if model has not been trained yet.
</span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
        test_features <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>add_bias_term(test_features)
        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>trained_weights <span style=color:#f92672>is</span> None:
            <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>&#34;You haven&#39;t trained the SVM yet!&#34;</span>)
            
        predicted_labels <span style=color:#f92672>=</span> []
        n_samples <span style=color:#f92672>=</span> test_features<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]
        <span style=color:#66d9ef>for</span> idx <span style=color:#f92672>in</span> range(n_samples):
            prediction <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sign(np<span style=color:#f92672>.</span>dot(self<span style=color:#f92672>.</span>trained_weights<span style=color:#f92672>.</span>T, test_features[idx]))
            predicted_labels<span style=color:#f92672>.</span>append(prediction)
            
        <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>array(predicted_labels)
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># Compute some values to make sure the cost is computed correctly</span>
<span style=color:#75715e># I calculated the values for this example by hand first</span>
svm <span style=color:#f92672>=</span> LinearSVM(regularization_param<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
weights <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>])[:, np<span style=color:#f92672>.</span>newaxis]
features <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>0.5</span>], [<span style=color:#ae81ff>2.5</span>]])
new_features <span style=color:#f92672>=</span> svm<span style=color:#f92672>.</span>add_bias_term(features)

labels <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>])
<span style=color:#66d9ef>assert</span> svm<span style=color:#f92672>.</span>compute_cost(weights, new_features, labels) <span style=color:#f92672>==</span> <span style=color:#ae81ff>4.</span>
gradient <span style=color:#f92672>=</span> svm<span style=color:#f92672>.</span>compute_gradient(weights, new_features, labels[:, np<span style=color:#f92672>.</span>newaxis])
</code></pre></div><h3 id=53-training-and-testing-an-svm>5.3 Training and testing an SVM</h3><p>After defining our SVM class we can train a model and test it on unseen examples.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># Initialize a new SVM and train it on the given toy dataset</span>
regularization_param <span style=color:#f92672>=</span> <span style=color:#ae81ff>100</span>
lr <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.000001</span>
svm <span style=color:#f92672>=</span> LinearSVM(regularization_param)
trained_weights <span style=color:#f92672>=</span> svm<span style=color:#f92672>.</span>train(features_train, labels_train, n_epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, learning_rate<span style=color:#f92672>=</span>lr)
</code></pre></div><pre><code>Epoch 1, cost: 28.999523042690537
Epoch 2, cost: 8.626987624824444
Epoch 3, cost: 3.8253539901075273
Epoch 4, cost: 2.4947875220684446
Epoch 5, cost: 1.9543704229309797
Epoch 6, cost: 1.5987135095492675
Epoch 7, cost: 1.3611077210579672
Epoch 8, cost: 1.1702119735961691
Epoch 9, cost: 1.0372585586110663
Epoch 10, cost: 0.9557095984777207
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># Predict lables for unknown test samples</span>
<span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> accuracy_score, recall_score, precision_score

predicted_labels <span style=color:#f92672>=</span> svm<span style=color:#f92672>.</span>predict(features_test)
predicted_labels <span style=color:#f92672>=</span> predicted_labels<span style=color:#f92672>.</span>flatten()

<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Accuracy on test dataset: {}&#34;</span><span style=color:#f92672>.</span>format(accuracy_score(labels_test, predicted_labels)))
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Recall on test dataset: {}&#34;</span><span style=color:#f92672>.</span>format(recall_score(labels_test, predicted_labels)))
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Precision on test dataset: {}&#34;</span><span style=color:#f92672>.</span>format(recall_score(labels_test, predicted_labels)))    
</code></pre></div><pre><code>Accuracy on test dataset: 1.0
Recall on test dataset: 1.0
Precision on test dataset: 1.0
</code></pre><h3 id=54-visualizing-the-decision-boundary>5.4 Visualizing the decision boundary</h3><p>Given our trained model we can visualize the decision boundary, as done below.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> numpy <span style=color:#f92672>as</span> np

<span style=color:#75715e># Create dataset for visualization</span>
size<span style=color:#f92672>=</span><span style=color:#ae81ff>40000</span>
feat_1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>uniform(low<span style=color:#f92672>=-</span><span style=color:#ae81ff>6</span>, high<span style=color:#f92672>=</span><span style=color:#ae81ff>7</span>, size<span style=color:#f92672>=</span>size)
feat_2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>uniform(low<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>, high<span style=color:#f92672>=</span><span style=color:#ae81ff>13</span>, size<span style=color:#f92672>=</span>size)
features_vis <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>column_stack((feat_1, feat_2))

labels_vis <span style=color:#f92672>=</span> svm<span style=color:#f92672>.</span>predict(features_vis)

<span style=color:#75715e># Plot the decision boundary</span>
plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>7</span>, <span style=color:#ae81ff>5</span>))
plt<span style=color:#f92672>.</span>scatter(features_vis[:, <span style=color:#ae81ff>0</span>], features_vis[:, <span style=color:#ae81ff>1</span>], c <span style=color:#f92672>=</span> labels_vis)
plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;SVM decision boundary&#34;</span>)
plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#34;Feature 2&#34;</span>)
plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#34;Feature 1&#34;</span>)
plt<span style=color:#f92672>.</span>show()
</code></pre></div><img src=/posts/machine_learning/images/svm_decision_boundary.png width=55% class=center><h2 id=6-dual-approach>6. Dual approach</h2><p>In the previous sections we took a detailed look at the primal SVM. To solve a primal SVM, we need to find the best values for the weights and bias. Recall that our input examples $\mathbf{x} \in \mathbb{R}^D$ have $D$ features. Consequently, our weights $\mathbf{w}$ have $D$ features, too. This can become problematic if the number of features $D$ is large.</p><p>That&rsquo;s where the second way of formalizing SVMs (called <em>dual approach</em>) comes in handy. The optimization problem of the dual approach is independent of the number of features. Instead, the number of parameters increases with the number of examples in the training set.</p><p>The dual approach uses the method of Lagrange multipliers. Lagrange multipliers allow us to find the minimum or maximum of a function if there are one or more constraints on the input values we are allowed to used.</p><p>If you never heard of Lagrange multipliers I can recommend the <a href=https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/lagrange-multipliers-single-constraint>blog posts</a> and <a href="https://www.youtube.com/watch?v=yuqB-d5MjZA&list=PLCg2-CTYVrQvNGLbd-FN70UxWZSeKP4wV&index=1">video tutorials</a> on the topic from Khan Academy.</p><h3 id=61-recap-lagrange-multipliers>6.1 Recap Lagrange multipliers</h3><p>Lagrange multipliers allow us to solve constrained optimization problems. Let&rsquo;s say we want to maximize the function $f(x, y) = 2x + y$ under the constraint that our values of $x$ and $y$ satisfy the following equation: $g(x, y) = x^2 + y^2 = 1$. This constraint equation describes a circle of radius 1.</p><p>The key insight behind the solution to this problem is that we need to find those values for $x$ and $y$ where the gradients of $f$ and $g$ are aligned. This can be expressed using a Lagrange multiplier (typically $\lambda$):</p><p>We want to find those values $x_m, y_m$ where $\nabla f(x_m, y_m) = \lambda \nabla g(x_m, y_m)$.</p><p>In our example the gradient vectors look as follows:
$$ \nabla f(x, y)=\left[\begin{array}{c}
\frac{\partial}{\partial x}(2 x+y) \\\<br>\frac{\partial}{\partial y}(2 x+y)
\end{array}\right]=\left[\begin{array}{c}
2 \\\ 1
\end{array}\right]$$</p><p>$$ \nabla g(x, y)=\left[\begin{array}{c}
\frac{\partial}{\partial x}\left(x^{2}+y^{2}-1\right) \\\<br>\frac{\partial}{\partial y}\left(x^{2}+y^{2}-1\right)
\end{array}\right]=\left[\begin{array}{c}
2 x \\\ 2 y
\end{array}\right]$$</p><p>Therefore, the tangency condition results in
$$ \left[\begin{array}{l}
2 \\\ 1
\end{array}\right]=\lambda \left[\begin{array}{l}
2 x_{m} \\\ 2 y_{m}
\end{array}\right] $$</p><p>We can rewrite the vector form into individual equations that can be solved by hand:</p><ul><li>$2 = \lambda 2 x_m $</li><li>$1 = \lambda 2 y_m $</li><li>$x_m^2 + y_m^2 = 1 $</li></ul><p>Solving the equations yields
$$ \begin{aligned}
\left(x_{0}, y_{0}\right) &=\left(\frac{1}{\lambda_{0}}, \frac{1}{2 \lambda_{0}}\right) \\\<br>&=\left(\frac{2}{\sqrt{5}}, \frac{1}{\sqrt{5}}\right) \quad \text { or } \quad\left(\frac{-2}{\sqrt{5}}, \frac{-1}{\sqrt{5}}\right)
\end{aligned} $$</p><p>where the first point denotes a maximum (what we wanted to find) and the second a minimum. This solves our constrained optimization problem. For more details and a full solution look at <a href=https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/lagrange-multipliers-single-constraint>this Khan academy post</a>.</p><h3 id=62-recap-lagrangian>6.2 Recap Lagrangian</h3><p>The Lagrangian is a way to repackage the individual conditions of our constrained optimization problem into a single equation. In the example above we wanted to optimize some function $f(x, y)$ under the constraint that the inputs $x$ and $y$ satisfy the equation $g(x, y) = x^2 + y^2 = c$. In our case the constant $c$ was given by 1. We know that the solution is given by those points where the gradients of $f$ and $g$ align. The Lagrangian function puts all of this into a single equation:</p><p>$$ \mathcal{L}(x, y, \lambda)=f(x, y)-\lambda(g(x, y)-c) $$</p><p>When computing the partial derivatives of $\mathcal{L}$ with respect to $x, y$ and $\lambda$ and setting them to zero, we will find that they correspond exactly to the three constraints we looked at earlier. This can be summarized by simply setting the gradient of $\mathcal{L}$ equal to the zero vector: $\nabla \mathcal{L} = \mathbf{0}$. The compact Lagrangian form is often used when solving constrained optimization problem with computers because it summarizes the elaborate problem with multiple constraints into a single equation. For more details and a full solution look at <a href=https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/lagrange-multipliers-single-constraint>this Khan academy post</a>.</p><h3 id=63-dual-optimization-problem>6.3 Dual optimization problem</h3><p>For the primal soft-margin SVM we considered the following optimization problem:
$$ \min_{\mathbf{w}, b, \mathbf{\xi}} \frac{1}{2} \Vert \mathbf{w} \Vert^2 + C \sum_{n=1}^N \xi_n $$</p><p>$$ \text{subject to:} $$</p><p>$$ y_n(\langle \mathbf{w}, \mathbf{x}_n \rangle + b) \ge 1 - \xi_n $$</p><p>$$ \xi_i \gt 0 \text{ for all } n = 1, &mldr;, N $$</p><p>To derive the corresponding Lagrangian we will introduce two Lagrange multipliers: $\alpha_n$ for the first constraint (that all examples are classified correctly) and $\lambda_n$ for the second constraint (non-negativity of the slack variables). The Lagrangian is then given by:</p><p>$$
\begin{aligned}
\mathfrak{L}(\boldsymbol{w}, b, \xi, \alpha, \gamma)=& \frac{1}{2}|\boldsymbol{w}|^{2}+C \sum_{n=1}^{N} \xi_{n} \\\<br>& \underbrace{-\sum_{n=1}^{N} \alpha_{n}\left(y_{n}\left(\left\langle\boldsymbol{w}, \boldsymbol{x}_{n}\right\rangle+b\right)-1+\xi_{n}\right)}_{\text{first constraint}} \underbrace{-\sum_{n=1}^{N} \gamma_{n} \xi_{n}}_{\text{second constraint}}
\end{aligned}
$$</p><p>Next, we have to compute the partial derivatives of the Lagrangian with respect to the variables $\mathbf{w}, b$ and $\xi$: $\frac{\partial \mathfrak{L}}{\partial \mathbf{w}}, \frac{\partial \mathfrak{L}}{\partial b}, \frac{\partial \mathfrak{L}}{\partial \xi}$. When setting the first partial derivative to zero we obtain an important interim result:
$$ \mathbf{w} = \sum_{n=1}^N \alpha_n y_n \mathbf{x}_n $$</p><p>This equation states the the optimal solution for the weight vector is given by a linear combination of our training examples. After setting the other partial derivatives to zero, using the result and simplifying the equations we end up with the following optimization problem (for details see section 12.3.1 of the <a href=https://mml-book.com>Mathematics for Machine Learning book</a>):</p><p>$$\min_{\boldsymbol{\alpha}} \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} y_{i} y_{j} \alpha_{i} \alpha_{j}\left\langle\mathbf{x}_{i}, \mathbf{x}_{j}\right\rangle-\sum_{i=1}^{N} \alpha_{i}$$
$$ \text{subject to:} $$
$$\sum_{i=1}^{N} y_{i} \alpha_{i}=0$$
$$0 \le \alpha_{i} \le C \text{ for all } i=1, \ldots, N$$</p><p>This constrained quadratic optimization problem can be solved very efficiently, for example with quadratic programming techniques. One popular library for solving dual SVMs is <a href=https://github.com/cjlin1/libsvm>libsvm</a> which makes use of a decomposition method to solve the problem (see <a href=https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>this paper</a> for more details). However, several other approaches exist.</p><h2 id=7-primal-vs-dual-approach>7. Primal vs. dual approach</h2><p>Most SVM research in the last decade has been about the dual formulation. Why this is the case is not clear. Both approaches have advantages and disadvantages. In the paper <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.129.3368&rep=rep1&type=pdf">&ldquo;Training a Support Vector Machine in the Primal&rdquo;</a> Chapelle et al. mention the following hypothesis:</p><blockquote><p>We believe that it is because SVMs were first introduced in their hard margin formulation (Boser et al., 1992), for which a dual optimization (because of the constraints) seems more natural. In general, however, soft margin SVMs should be preferred, even if the training data are separable: the decision boundary is more robust because more training points are taken into account (Chapelle et al., 2000). We do not pretend that primal optimization is better in general; our main motivation wasto point out that primal and dual are two sides of the same coin and that there is no reason to look always at the same side.</p></blockquote><h2 id=8-kernels--non-linear-svm>8. Kernels / non-linear SVM</h2><h3 id=81-what-is-a-kernel>8.1 What is a kernel?</h3><p>If you take another look at the optimization equation of dual SVMs you will notice that it computes the inner product $\left\langle\mathbf{x}_{i}, \mathbf{x}_{j}\right\rangle$ between all datapoints $\mathbf{x}_{i}, \mathbf{x}_{j}$. A kernel is a way to compute this inner product implicitely in some (potentially very high dimensional) feature space. To be more precise: assume we have some mapping function $\varphi$ which maps an $n$ dimensional input vector to an $m$ dimensional output vector: $\varphi , : , \mathbb R^n \to \mathbb R^m$. Given this mapping function we can compute the dot product of two vectors $\mathbf x$ and $\mathbf y$ in this space as follows: $\varphi(\mathbf x)^T \varphi(\mathbf y)$.</p><p>A kernel is a function $k$ that gives the same result as this dot product: $k(\mathbf x, \mathbf y) = \varphi(\mathbf x)^T \varphi(\mathbf y)$. In other words: the kernel function is equivalent to the dot product of the mapping function.</p><h3 id=82-what-are-kernels-good-for>8.2 What are kernels good for?</h3><p>Until now (apart from the soft-margin SVM) our SVMs, both primal and dual, are only able to classify data that is <a href=https://en.wikipedia.org/wiki/Linear_separability>linearly separable</a>. However, most datasets in practice won&rsquo;t be of this form. We need a way to classify data that is <strong>not</strong> linearly separable. This is where the so called <strong>kernel trick</strong> comes into play.</p><p>Because the objective function of the dual SVM contains inner products only between datapoints $\mathbf{x}_i, \mathbf{x}_j$, we can easily replace this inner product (that is, $\left\langle\mathbf{x}_{i}, \mathbf{x}_{j}\right\rangle$ ) with some mapping function $\varphi(\mathbf{x}_i)^T \varphi(\mathbf{x}_j)$. This mapping function can be non-linear, allowing us to compute an SVM that is non-linear with respect to the input examples. The mapping function takes our input data (which is not linearly separable) and transforms it into some higher-dimensional space where it becomes linearly separable. This is illustrated in the figure below.</p><img src=/posts/machine_learning/images/feature_mapping_illustration.png width=60% class=center><p>In theory, we could use any mapping function we like. In practice, however, computing inner products is expensive. Therefore, we use mapping functions that have a corresponding kernel function. This will allow us to map the datapoints into a higher dimensional space without ever explicitely computing the (expensive) inner products.</p><p>Let&rsquo;s take a look at an example.</p><h3 id=83-example>8.3 Example</h3><p>Note: this example was taken from <a href=https://stats.stackexchange.com/posts/153134>this StackExchange post</a>.</p><p>We can create a simple polynomial kernel as follows: $k(\mathbf{x}, \mathbf{y}) = (1 + \mathbf x^T \mathbf y)^2$ with $\mathbf x, \mathbf y \in \mathbb R^2$. The kernel does not seem to correspond to any mapping function $\varphi$, it&rsquo;s just a function that returns a real number. Our input vectors $\mathbf{x}, \mathbf{y}$ are 2-dimensional: $\mathbf x = (x_1, x_2)$ and $\mathbf y = (y_1, y_2)$. With this knowledge we can expand the kernel computation:</p><p>$\begin{align}
k(\mathbf x, \mathbf y) & = (1 + \mathbf x^T \mathbf y)^2 \\\<br>&= (1 + x_1 , y_1 + x_2 , y_2)^2 \\\<br>& = 1 + x_1^2 y_1^2 + x_2^2 y_2^2 + 2 x_1 y_1 + 2 x_2 y_2 + 2 x_1 x_2 y_1 y_2
\end{align}$</p><p>Note that this is nothing else but a dot product between two vectors $(1, x_1^2, x_2^2, \sqrt{2} x_1, \sqrt{2} x_2, \sqrt{2} x_1 x_2)$ and $(1, y_1^2, y_2^2, \sqrt{2} y_1, \sqrt{2} y_2, \sqrt{2} y_1 y_2)$. This can be expressed with the following mapping function:
$$\varphi(\mathbf x) = \varphi(x_1, x_2) = (1, x_1^2, x_2^2, \sqrt{2} x_1, \sqrt{2} x_2, \sqrt{2} x_1 x_2)$$</p><p>So the kernel $k(\mathbf x, \mathbf y) = (1 + \mathbf x^T \mathbf y)^2 = \varphi(\mathbf x)^T \varphi(\mathbf y)$ computes a dot product in 6-dimensional space without explicitly visiting this space. The generalization from an inner product to a kernel function is known as the <strong>kernel trick</strong>.</p><p>Several popular kernel functions exist. Popular ones are, for example, the <a href=https://en.wikipedia.org/wiki/Polynomial_kernel>polyomial kernel</a> or <a href=https://en.wikipedia.org/wiki/Radial_basis_function_kernel>RBF kernel</a>.</p><h3 id=84-can-we-also-use-kernels-in-the-primal-svm>8.4 Can we also use kernels in the primal SVM?</h3><p>Yes, the kernel trick can be applied to primal SVM&rsquo;s, too. It&rsquo;s not as straightforward as with dual SVMs but still possible. Consider <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.129.3368&rep=rep1&type=pdf">this paper by Chapelle et al.</a> as an example.</p><h3 id=85-how-do-i-choose-the-right-kernel-for-my-problem>8.5 How do I choose the right kernel for my problem?</h3><p>The problem of choosing the right kernel has been answered in <a href=https://stats.stackexchange.com/questions/18030/how-to-select-kernel-for-svm>this StackExchange post</a>.</p><p>Summary:</p><ul><li>Without expert knowledge, the Radial Basis Function kernel makes a good default kernel (in case you need a non-linear model)</li><li>The choice of the kernel and parameters can be automated by optimising a cross-validation based model selection</li><li>Choosing the kernel and parameters automatically is tricky, as it is very easy to overfit the model selection criterion</li></ul><h2 id=9-sources-and-further-reading>9. Sources and further reading</h2><p>The basis for this notebook is chapter 12 of the book <a href=https://mml-book.github.io/>Mathematics for Machine Learning</a>. I can highly recommend to read through the entire chapter to get a deeper understanding of support vector machines.</p><p>Another source I liked very much is <a href="https://www.youtube.com/watch?v=_PwhiWxHK8o">this MIT lecture on SVMs</a>.</p></div><div class=btn-improve-page><a href=https://github.com/zotroneneis/zotroneneis.github.io/edit/main/content/posts/machine_learning/support_vector_machines.md><i class="fas fa-code-branch"></i>
Improve this page</a></div><hr><div class="row next-prev-navigator"><div class="col-md-12 next-article"><a href=/posts/books/reading_list/ class="btn btn-outline-info"><span>Next <i class="fas fa-chevron-circle-right"></i></span><br><span>Personal reading list of non-fiction books</span></a></div></div><hr></div></div></div></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#1-what-are-support-vector-machines>1. What are support vector machines?</a></li><li><a href=#2-primal-approach---hard-margin-svm>2. Primal approach - Hard-margin SVM</a><ul><li><a href=#21-goal-1>2.1 Goal 1</a></li><li><a href=#22-goal-2>2.2 Goal 2</a></li><li><a href=#23-combined-goal>2.3 Combined goal</a></li><li><a href=#24-optional-deriving-the-margin-equation>2.4 (Optional) Deriving the margin equation</a></li></ul></li><li><a href=#3-primal-approach---soft-margin-svm>3. Primal approach - Soft-margin SVM</a></li><li><a href=#4-solving-the-primal-optimization-problem>4. Solving the primal optimization problem</a><ul><li><a href=#41-hinge-loss-function>4.1 Hinge loss function</a></li><li><a href=#42-updated-objective-function>4.2 Updated objective function</a></li><li><a href=#43-optional-three-parts-of-the-objective-function>4.3 [Optional] Three parts of the objective function</a></li><li><a href=#44-sub-gradient-descent>4.4 Sub-gradient descent</a></li><li><a href=#45-optional-difference-subgradient-descent-and-gradient-descent>4.5 [Optional] Difference subgradient descent and gradient descent</a><ul><li><a href=#451-what-is-a-subgradient>4.5.1 What is a subgradient?</a></li><li><a href=#452-subgradient-method>4.5.2 Subgradient method</a></li></ul></li></ul></li><li><a href=#5-implementation-primal-approach>5. Implementation primal approach</a><ul><li><a href=#51-toy-dataset>5.1 Toy dataset</a></li><li><a href=#52-svm-class-definition>5.2 SVM class definition</a></li><li><a href=#53-training-and-testing-an-svm>5.3 Training and testing an SVM</a></li><li><a href=#54-visualizing-the-decision-boundary>5.4 Visualizing the decision boundary</a></li></ul></li><li><a href=#6-dual-approach>6. Dual approach</a><ul><li><a href=#61-recap-lagrange-multipliers>6.1 Recap Lagrange multipliers</a></li><li><a href=#62-recap-lagrangian>6.2 Recap Lagrangian</a></li><li><a href=#63-dual-optimization-problem>6.3 Dual optimization problem</a></li></ul></li><li><a href=#7-primal-vs-dual-approach>7. Primal vs. dual approach</a></li><li><a href=#8-kernels--non-linear-svm>8. Kernels / non-linear SVM</a><ul><li><a href=#81-what-is-a-kernel>8.1 What is a kernel?</a></li><li><a href=#82-what-are-kernels-good-for>8.2 What are kernels good for?</a></li><li><a href=#83-example>8.3 Example</a></li><li><a href=#84-can-we-also-use-kernels-in-the-primal-svm>8.4 Can we also use kernels in the primal SVM?</a></li><li><a href=#85-how-do-i-choose-the-right-kernel-for-my-problem>8.5 How do I choose the right kernel for my problem?</a></li></ul></li><li><a href=#9-sources-and-further-reading>9. Sources and further reading</a></li></ul></nav></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=#about>About</a></li><li class=nav-item><a class=smooth-scroll href=#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=#education>Education</a></li><li class=nav-item><a class=smooth-scroll href=#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=#appearances>Talks & Podcasts</a></li><li class=nav-item><a class=smooth-scroll href=#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span>Email:</span> <span>popkes@gmx.net</span></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=#><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png>
Toha</a></div><div class="col-md-4 text-center"> 2020-2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/js/jquery-3.4.1.min.js></script><script src=/js/popper.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/navbar.js></script><script src=/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],processEnvironments:!0}}</script></body></html>