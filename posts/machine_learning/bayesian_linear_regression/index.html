<!doctype html><html lang=en><head><title>Bayesian linear regression</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/application.c27cd4e332ca297fb25625ebfb0fd08824b3344ee00869c5bc395be617ba225b.css integrity="sha256-wnzU4zLKKX+yViXr+w/QiCSzNE7gCGnFvDlb5he6Ils="><link rel=icon type=image/png href=/images/author/image_al_hu15845133475761513827.jpg><meta property="og:url" content="https://alpopkes.com/posts/machine_learning/bayesian_linear_regression/"><meta property="og:site_name" content="Anna-Lena Popkes"><meta property="og:title" content="Bayesian linear regression"><meta property="og:description" content="I finally found time to continue working on my machine learning basics repository which implements fundamental machine learning algorithms in plain Python. Especially, I took a detailed look at Bayesian linear regression. The blog post below contains the same content as the original notebook. You can run the notebook directly in your Browser using Binder.
1. What is Bayesian linear regression (BLR)? Bayesian linear regression is the Bayesian interpretation of linear regression."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-02-20T00:00:00+00:00"><meta property="article:modified_time" content="2021-02-20T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Bayesian linear regression"><meta name=twitter:description content="I finally found time to continue working on my machine learning basics repository which implements fundamental machine learning algorithms in plain Python. Especially, I took a detailed look at Bayesian linear regression. The blog post below contains the same content as the original notebook. You can run the notebook directly in your Browser using Binder.
1. What is Bayesian linear regression (BLR)? Bayesian linear regression is the Bayesian interpretation of linear regression."><meta name=description content="Bayesian linear regression"><script>theme=localStorage.getItem("darkmode:color-scheme")||"system",theme=="system"&&(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?theme="dark":theme="light"),document.documentElement.setAttribute("data-theme",theme)</script></head><body class="type-posts kind-page" data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-secondary wrapper"><nav class="navbar navbar-expand-xl top-navbar shadow" id=top-navbar><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button>
<i data-feather=sidebar></i>
</button>
<a class=navbar-brand href=/><img src=/images/author/image_al_hu15845133475761513827.jpg id=logo alt=Logo>
Anna-Lena Popkes</a>
<button class="navbar-toggler navbar-light" id=navbar-toggler type=button data-toggle=collapse data-target=#top-nav-items aria-label=menu>
<i data-feather=menu></i></button><div class="collapse navbar-collapse dynamic-navbar" id=top-nav-items><ul class="nav navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/#home>Home</a></li><li class=nav-item><a class=nav-link href=/#about>About</a></li><li class=nav-item><a class=nav-link href=/#skills>Skills</a></li><li class=nav-item><a class=nav-link href=/#experiences>Experiences</a></li><li class=nav-item><a class=nav-link href=/#education>Education</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>More</a><div class=dropdown-menu aria-labelledby=navbarDropdown><a class=dropdown-item href=/#projects>Projects</a>
<a class=dropdown-item href=/#recent-posts>Recent Posts</a>
<a class=dropdown-item href=/#appearances>Talks & Podcasts</a></div></li><div id=top-navbar-divider></div><li class=nav-item><a class=nav-link id=blog-link href=/posts>Posts</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg class=theme-icon src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme"></a><div id=themeMenu class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# data-scheme=light><img class=theme-icon src=/icons/sun-svgrepo-com.svg width=20 alt="Light Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=dark><img class=theme-icon src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=system><img class=theme-icon src=/icons/computer-svgrepo-com.svg width=20 alt="System Theme"></a></div></li></ul></div></div><img src=/images/author/image_al_hu15845133475761513827.jpg class=d-none id=main-logo alt=Logo>
<img src=/images/author/image_al_hu15845133475761513827.jpg class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts/ data-filter=all>Posts</a></li><div class=subtree><li><a class=list-link href=/posts/news/ title="Personal news">Personal news</a></li><li><a class=list-link href=/posts/my_path_to_ml/ title="My path to machine learning">My path to machine learning</a></li><li><i data-feather=plus-circle></i><a class=list-link href=/posts/books/> Books</a><ul><li><a class=list-link href=/posts/books/reading_list/ title="Personal reading List">Personal reading List</a></li><li><a class=list-link href=/posts/books/deep_work/ title="Deep work">Deep work</a></li></ul></li><li><i data-feather=minus-circle></i><a class="active list-link" href=/posts/machine_learning/> Machine Learning</a><ul class=active><li><a class="active list-link" href=/posts/machine_learning/bayesian_linear_regression/ title="Bayesian linear regression">Bayesian linear regression</a></li><li><a class=list-link href=/posts/machine_learning/bayesian_linear_regression_2/ title="Bayesian linear regression 2">Bayesian linear regression 2</a></li><li><a class=list-link href=/posts/machine_learning/kl_divergence/ title="KL Divergence">KL Divergence</a></li><li><a class=list-link href=/posts/machine_learning/principal_component_analysis/ title="Principal component analysis (PCA)">Principal component analysis (PCA)</a></li><li><a class=list-link href=/posts/machine_learning/support_vector_machines/ title="Support vector machines">Support vector machines</a></li><li><a class=list-link href=/posts/machine_learning/variational_inference/ title="Variational Inference">Variational Inference</a></li></ul></li><li><i data-feather=plus-circle></i><a class=list-link href=/posts/python/> Python</a><ul><li><a class=list-link href=/posts/python/coding_with_kids/ title="Coding with kids">Coding with kids</a></li><li><a class=list-link href=/posts/python/mocking/ title=Mocking>Mocking</a></li><li><a class=list-link href=/posts/python/packaging_tools/ title="Packaging tools">Packaging tools</a></li><li><i data-feather=plus-circle></i><a class=list-link href=/posts/python/magical_universe/> Magical Universe</a><ul><li><a class=list-link href=/posts/python/magical_universe/day_1_start/ title=Start>Start</a></li><li><a class=list-link href=/posts/python/magical_universe/day_1_magical_universe/ title="The Tales of Castle Kilmere">The Tales of Castle Kilmere</a></li><li><a class=list-link href=/posts/python/magical_universe/day_1_first_post_oop/ title="Object-oriented programming">Object-oriented programming</a></li><li><a class=list-link href=/posts/python/magical_universe/day_2_types_of_methods/ title="Types of methods">Types of methods</a></li><li><a class=list-link href=/posts/python/magical_universe/day_3_type_annotations/ title="Type annotations">Type annotations</a></li><li><a class=list-link href=/posts/python/magical_universe/day_4_to_string_conversion/ title="To-string conversion">To-string conversion</a></li><li><a class=list-link href=/posts/python/magical_universe/day_5_decorators/ title=Decorators>Decorators</a></li><li><a class=list-link href=/posts/python/magical_universe/day_6_properties/ title=Properties>Properties</a></li><li><a class=list-link href=/posts/python/magical_universe/day_7_underscore_patterns/ title="Underscore patterns">Underscore patterns</a></li><li><a class=list-link href=/posts/python/magical_universe/day_8_extending_universe/ title="Extending the universe">Extending the universe</a></li><li><a class=list-link href=/posts/python/magical_universe/day_9_duck_typing/ title="Duck Typing">Duck Typing</a></li><li><a class=list-link href=/posts/python/magical_universe/day_10_11_namedtuples/ title=Namedtuples>Namedtuples</a></li><li><a class=list-link href=/posts/python/magical_universe/day_12_to_15_abcs/ title="Abstract Base Classes">Abstract Base Classes</a></li><li><a class=list-link href=/posts/python/magical_universe/day_16_to_18_data_classes/ title="Data classes">Data classes</a></li><li><a class=list-link href=/posts/python/magical_universe/day_19_immutable_data_classes/ title="Immutable data classes">Immutable data classes</a></li><li><a class=list-link href=/posts/python/magical_universe/day_20_decorators_in_classes/ title="Decorators in classes">Decorators in classes</a></li><li><a class=list-link href=/posts/python/magical_universe/day_21_if_main/ title='if __name__ == "__main__"'>if __name__ == "__main__"</a></li><li><a class=list-link href=/posts/python/magical_universe/day_22_to_24_context_managers/ title="Context managers">Context managers</a></li><li><a class=list-link href=/posts/python/magical_universe/day_25_to_28_pytest/ title="Testing with pytest">Testing with pytest</a></li><li><a class=list-link href=/posts/python/magical_universe/day_29_to_31_iterators/ title=Iterators>Iterators</a></li><li><a class=list-link href=/posts/python/magical_universe/day_34_multisets/ title=Multisets>Multisets</a></li><li><a class=list-link href=/posts/python/magical_universe/day_37_extending_universe/ title="Extending the universe II">Extending the universe II</a></li><li><a class=list-link href=/posts/python/magical_universe/day_43_to_45_exception_classes/ title="Exception classes">Exception classes</a></li><li><a class=list-link href=/posts/python/magical_universe/day_46_functools_wraps/ title=functools.wraps>functools.wraps</a></li><li><a class=list-link href=/posts/python/magical_universe/day_47_to_48_defaultdict/ title=Defaultdict>Defaultdict</a></li><li><a class=list-link href=/posts/python/magical_universe/day_49_to_50_config_files/ title="Config files">Config files</a></li><li><a class=list-link href=/posts/python/magical_universe/2018-09-16-blog-post-day-51/ title="Wrap up">Wrap up</a></li></ul></li></ul></li><li><i data-feather=plus-circle></i><a class=list-link href=/posts/software_engineering/> Software Engineering</a><ul><li><a class=list-link href=/posts/software_engineering/containers/ title="Intro to containers">Intro to containers</a></li><li><a class=list-link href=/posts/software_engineering/docker/ title="Intro to Docker">Intro to Docker</a></li><li><a class=list-link href=/posts/software_engineering/virtual_machines/ title="Intro to virtual machines">Intro to virtual machines</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/image_al_hu6187147171234605626.jpg alt="Author Image"><h5 class=author-name>Anna-Lena Popkes</h5><p class=text-muted>Saturday, February 20, 2021</p></div><div class=title><h1>Bayesian linear regression</h1></div><div class=post-content id=post-content><p>I finally found time to continue working on my <a href=https://github.com/zotroneneis/machine_learning_basics target=_blank rel=noopener>machine learning basics repository</a> which implements fundamental machine learning algorithms in plain Python. Especially, I took a detailed look at Bayesian linear regression. The blog post below contains the same content as the <a href=https://github.com/zotroneneis/machine_learning_basics/blob/master/bayesian_linear_regression.ipynb target=_blank rel=noopener>original notebook</a>. You can <a href="https://mybinder.org/v2/gh/zotroneneis/machine_learning_basics/HEAD?filepath=bayesian_linear_regression.ipynb" target=_blank rel=noopener>run the notebook directly in your Browser using Binder</a>.</p><h2 id=1-what-is-bayesian-linear-regression-blr>1. What is Bayesian linear regression (BLR)?</h2><p>Bayesian linear regression is the <em>Bayesian</em> interpretation of linear regression. What does that mean? To answer this question we first have to understand the Bayesian approach. In most of the algorithms we have looked at so far we computed <em>point estimates</em> of our parameters. For example, in linear regression we chose values for the weights and bias that minimized our mean squared error cost function. In the Bayesian approach we don&rsquo;t work with exact values but with <em>probabilities</em>. This allows us to model the <em>uncertainty</em> in our parameter estimates. Why is this important?</p><p>In nearly all real-world situations, our data and knowledge about the world is incomplete, indirect and noisy. Hence, uncertainty must be a fundamental part of our decision-making process. This is exactly what the Bayesian approach is about. It provides a formal and consistent way to reason in the presence of uncertainty. Bayesian methods have been around for a long time and are widely-used in many areas of science (e.g. astronomy). Although Bayesian methods have been applied to machine learning problems too, they are usually less well known to beginners. The major reason is that they require a good understanding of probability theory.</p><p>In the following post we will work our way from linear regression to Bayesian linear regression, including the most important theoretical knowledge and code examples. Remember: you can <a href="https://mybinder.org/v2/gh/zotroneneis/machine_learning_basics/HEAD?filepath=bayesian_linear_regression.ipynb" target=_blank rel=noopener>run the original notebook directly in your Browser using Binder</a>.</p><h2 id=2-recap-linear-regression>2. Recap linear regression</h2><ul><li>In linear regression, we want to find a function $f$ that maps inputs $x \in \mathbb{R}^D$ to corresponding function values $f(x) \in \mathbb{R}$.</li><li>We are given an input dataset $D = \Big \{ \mathbf{x}_n, y_n \Big \}_{n=1}^N$, where $y_n$ is a noisy observation value: $y_n = f(x_n) + \epsilon$, with $\epsilon$ being an i.i.d. random variable that describes measurement/observation noise</li><li>Our goal is to infer the underlying function $f$ that generated the data such that we can predict function values at new input locations</li><li>In linear regression, we model the underlying function $f$ using a linear combination of the input features:</li></ul><p>$$
\begin{split}
y &= \theta_0 + \theta_1 x_1 + \theta_2 x_2 + &mldr; + \theta_d x_d \\\
&= \pmb{x}^T \pmb{\theta}
\end{split}
$$</p><ul><li>For more details take a look at the <a href=https://github.com/zotroneneis/machine_learning_basics/blob/master/linear_regression.ipynb target=_blank rel=noopener>notebook on linear regression</a></li></ul><h2 id=3-fundamental-concepts>3. Fundamental concepts</h2><ul><li>One fundamental tool in Bayesian learning is <a href=https://en.wikipedia.org/wiki/Bayes%27_theorem target=_blank rel=noopener>Bayes&rsquo; theorem</a></li><li>Bayes&rsquo; theorem looks as follows:
$$
\begin{equation}
p(\pmb{\theta} | \mathbf{x}, y) = \frac{p(y | \pmb{x}, \pmb{\theta})p(\pmb{\theta})}{p(\pmb{x}, y)}
\end{equation}
$$</li><li>$p(y | \pmb{x}, \pmb{\theta})$ is the <em>likelihood</em>. It describes the probability of the target values given the data and parameters.</li><li>$p(\pmb{\theta})$ is the <em>prior</em>. It describes our initial knowledge about which parameter values are likely and unlikely.</li><li>$p(\pmb{x}, y)$ is the <em>evidence</em>. It describes the joint probability of the data and targets.</li><li>$p(\pmb{\theta} | \pmb{x}, y)$ is the <em>posterior</em>. It describes the probability of the parameters given the observed data and targets.</li><li>Another important tool you need to know about is the <a href=https://en.wikipedia.org/wiki/Normal_distribution target=_blank rel=noopener>Gaussian distribution</a>. If you are not familiar with it I suggest you pause for a minute and understand its main properties before reading on.</li></ul><p>The general Bayesian inference approach is as follows:</p><ol><li>We start with some prior belief about a hypothesis p(h)</li><li>We observe some data, gaining new evidence $e$</li><li>We update our belief using Bayes&rsquo; theorem, resulting</li></ol><p>Finally, we use Bayes&rsquo; theorem
$p(h|e) = \frac{p(e |h)p(h)}{p(e)}$
to incorporate the new evidence yielding a refined posterior belief</p><h2 id=4-linear-regression-from-a-probabilistic-perspective>4. Linear regression from a probabilistic perspective</h2><p>In order to pave the way for Bayesian linear regression we will take a probabilistic spin on linear regression. Let&rsquo;s start by explicitly modelling the observation noise $\epsilon$. For simplicity, we assume that $\epsilon$ is normally distributed with mean $0$ and some known variance $\sigma^2$: $\epsilon \sim \mathcal{N}(0, \sigma^2)$.</p><p>As mentioned in the beginning, a simple linear regression model assumes that the target function $f(x)$ is given by a linear combination of the input features:
$$
\begin{split}
y = f(\pmb{x}) + \epsilon \\\
= \pmb{x}^T \pmb{\theta} + \epsilon
\end{split}
$$</p><p>This corresponds to the following likelihood function:
$$p(y | \pmb{x}, \pmb{\theta}) = \mathcal{N}(\pmb{x}^T \pmb{\theta}, \sigma^2)$$</p><p>Our goal is to find the parameters $\pmb{\theta} = \Big \{ \theta_1, &mldr;, \theta_D \Big \}$ that model the given data best.
In standard linear regression we can find the best parameters using a least-squares, maximum likelihood (ML) or maximum a posteriori (MAP) approach. If you want to know more about these solutions take a look at the <a href=https://github.com/zotroneneis/machine_learning_basics/blob/master/linear_regression.ipynb target=_blank rel=noopener>notebook on linear regression</a> or at chapter 9.2 of the book <a href=https://mml-book.com target=_blank rel=noopener>Mathematics for Machine Learning</a>.</p><h2 id=5-linear-regression-with-basis-functions>5. Linear regression with basis functions</h2><p>The simple linear regression model above is linear not only with respect to the parameters $\pmb{\theta}$ but also with respect to the inputs $\pmb{x}$. When $\pmb{x}$ is not a vector but a single value (that is, the dataset is one-dimensional) the model $y_i = x_i \cdot \theta$ describes straight lines with $\theta$ being the slope of the line.</p><p>The plot below shows example lines produced with the model $y = x \cdot \theta$, using different values for the slope $\theta$ and intercept 0.</p><img src=/posts/machine_learning/images/example_straight_lines.png width=50% class=center><p>Having a model which is linear both with respect to the parameters and inputs limits the functions it can learn significantly. We can make our model more powerful by making it <em>nonlinear</em> with respect to the inputs. After all, <em>linear regression</em> refers to models which are linear in the <em>parameters</em>, not necessarily in the <em>inputs</em> (linear in the parameters means that the model describes a function by a linear combination of input features).</p><p>Making the model nonlinear with respect to the inputs is easy. We can adapt it by using a nonlinear transformation of the input features $\phi(\pmb{x})$. With this adaptation our model looks as follows:
$$
\begin{split}
y &= \pmb{\phi}^T(\pmb{x}) \pmb{\theta} + \epsilon \
&= \sum_{k=0}^{K-1} \theta_k \phi_k(\pmb{x}) + \epsilon
\end{split}
$$</p><p>Where $\pmb{\phi}: \mathbf{R}^D \rightarrow \mathbf{R}^K$ is a (non)linear transformation of the inputs $\pmb{x}$ and $\phi_k: \mathbf{R}^D \rightarrow \mathbf{R}$ is the $k-$th component of the <em>feature vector</em> $\pmb{\phi}$:</p><p>$$
\pmb{\phi}(\pmb{x})=\left[\begin{array}{c}
\phi_{0}(\pmb{x}) \\\
\phi_{1}(\pmb{x}) \\\
\vdots \\\
\phi_{K-1}(\pmb{x})
\end{array}
\right]
\in \mathbb{R}^{K}
$$</p><p>With our new nonlinear transformation the likelihood function is given by</p><p>$$
p(y | \pmb{x}, \pmb{\theta}) = \mathcal{N}(\pmb{\phi}^T(\pmb{x}) \pmb{\theta},, \sigma^2)
$$</p><h3 id=51-example-basis-functions>5.1 Example basis functions</h3><h4 id=linear-regression>Linear regression</h4><p>The easiest example for a basis function (for one-dimensional data) would be simple linear regression, that is, no non-linear transformation at all. In this case we would choose $\phi_0(x) = 1$ and $\phi_i(x) = x$. This would result in the following vector $\pmb{\phi}(x)$:</p><p>$$
\pmb{\phi}(x)=
\left[
\begin{array}{c}
\phi_{0}(x) \\\
\phi_{1}(x) \\\
\vdots \\\
\phi_{K-1}(x)
\end{array}
\right] =
\left[
\begin{array}{c}
1 \\\
x \\\
\vdots \\\
x
\end{array}
\right]
\in \mathbb{R}^{K}
$$</p><h4 id=polynomial-regression>Polynomial regression</h4><p>Another common choice of basis function for the one-dimensional case is polynomial regression. For this we would set $\phi_i(x) = x^i$ for $i=0, &mldr;, K-1$. The corresponding feature vector $\pmb{\phi}(x)$ would look as follows:</p><p>$$
\pmb{\phi}(x)=
\left[
\begin{array}{c}
\phi_{0}(x) \\\
\phi_{1}(x) \\\
\vdots \\\
\phi_{K-1}(x)
\end{array}
\right] =
\left[
\begin{array}{c}
1 \\\
x \\\
x^2 \\\
x^3 \\\
\vdots \\\
x^{K-1}
\end{array}
\right]
\in \mathbb{R}^{K}
$$</p><p>With this transformation we can lift our original one-dimensional input into a $K$-dimensional feature space. Our function $f$ can be any polynomial with degree $\le K-1$: $f(x) = \sum_{k=0}^{K-1} \theta_k x^k$</p><h3 id=52-the-design-matrix>5.2 The design matrix</h3><p>To make it easier to work with the transformations $\pmb{\phi}(\pmb{x})$ for the different input vectors $\pmb{x}$ we typically create a so called <em>design matrix</em> (also called <em>feature matrix</em>). Given our dataset $D = { \pmb{x_n}, y_n }_{n=1}^N$ we define the design matrix as follows:</p><p>$$
\boldsymbol{\Phi}:=\left[\begin{array}{c}
\boldsymbol{\phi}^{\top}\left(\boldsymbol{x}_{1}\right) \\\
\vdots \\\
\boldsymbol{\phi}^{\top}\left(\boldsymbol{x}_{N}\right)
\end{array}\right]=\left[\begin{array}{ccc}
\phi_{0}\left(\boldsymbol{x}_{1}\right) & \cdots & \phi_{K-1}\left(\boldsymbol{x}_{1}\right) \\\
\phi_{0}\left(\boldsymbol{x}_{2}\right) & \cdots & \phi_{K-1}\left(\boldsymbol{x}_{2}\right) \\\
\vdots & & \vdots \\\
\phi_{0}\left(\boldsymbol{x}_{N}\right) & \cdots & \phi_{K-1}\left(\boldsymbol{x}_{N}\right)
\end{array}\right] \in \mathbb{R}^{N \times K}
$$</p><p>Note that the design matrix is of shape $N \times K$. $N$ is the number of input examples and $K$ is the output dimension of the non-linear transformation $\pmb{\phi}(\pmb{x})$.</p><h2 id=6-bayesian-linear-regression>6. Bayesian linear regression</h2><p>What changes when we consider a Bayesian interpretation of linear regression? Our data stays the same as before: $D = \Big \{ \pmb{x_n}, y_n \Big \}_{n=1}^N$. Given the data $D$ we can define the set of all inputs as $\mathcal{X} := \Big \{ \pmb{x}_1, &mldr;, \pmb{x}_n \Big \}$ and the set of all targets as $\mathcal{Y} := \Big \{ y_1, &mldr;, y_n \Big \}$.</p><p>In simple linear regression we compute point estimates of our parameters (e.g. using a maximum likelihood approach) and use these estimates to make predictions. Different to this, Bayesian linear regression estimates <em>distributions</em> over the parameters and predictions. This allows us to model the uncertainty in our predictions.</p><p>To perform Bayesian linear regression we follow three steps:</p><ol><li>We set up a probabilistic model that describes our assumptions how the data and parameters are generated</li><li>We perform inference for the parameters $\pmb{\theta}$, that is, we compute the posterior probability distribution over the parameters</li><li>With this posterior we can perform inference for new, unseen inputs $y_{*}$. In this step we don&rsquo;t compute point estimates of the outputs. Instead, we compute the parameters of the posterior distribution over the outputs.</li></ol><h3 id=61-step-1-probabilistic-model>6.1 Step 1: Probabilistic model</h3><p>We start by setting up a probabilistic model that describes our assumptions how the data and parameters are generated. For this, we place a prior $p(\pmb{\theta})$ over our parameters which encodes what parameter values are plausible (before we have seen any data). Example: With a single parameter $\theta$, a Gaussian prior $p(\theta) = \mathcal{N}(0, 1)$ says that parameter values are normally distributed with mean 0 and standard deviation 1. In other words: the parameter values are most likely to fall into the interval $[−2,2]$ which is two standard deviations around the mean value.</p><p>To keep things simple we will assume a Gaussian prior over the parameters: $p(\pmb{\theta}) = \mathcal{N}(\pmb{m}_0, \pmb{S}_0)$. Let&rsquo;s further assume that the likelihood function is Gaussian, too: $p(y \mid \pmb{x}, \pmb{\theta})=\mathcal{N}\left(y \mid \pmb{\phi}^{\top}(\pmb{x}) \pmb{\theta}, \sigma^{2}\right)$.</p><p>Note: When considering the set of all targets $\mathcal{Y} := \Big \{ y_1, &mldr;, y_n \Big \}$, the likelihood function becomes a multivariate Gaussian distribution: $p(\mathcal{Y} \mid \mathcal{X}, \pmb{\theta})=\mathcal{N}\left(\pmb{y} \mid \pmb{\Phi} \pmb{\theta}, \sigma^{2} \pmb{I}\right)$</p><p>The nice thing about choosing a Gaussian distribution for our prior is that the posterior distributions will be Gaussian, too (keyword <a href=https://en.wikipedia.org/wiki/Conjugate_prior target=_blank rel=noopener>conjugate prior</a>)!</p><p>We will start our <code>BayesianLinearRegression</code> class with the knowledge we have so far - our probabilistic model. As mentioned in the beginning we assume that the variance $\sigma^2$ of the noise $\epsilon$ is known. Furthermore, to allow plotting the data later on we will assume that it&rsquo;s two dimensional (d=2).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> scipy.stats <span style=color:#f92672>import</span> multivariate_normal
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>BayesianLinearRegression</span>:
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34; Bayesian linear regression
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        prior_mean: Mean values of the prior distribution (m_0)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        prior_cov: Covariance matrix of the prior distribution (S_0)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        noise_var: Variance of the noise distribution
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, prior_mean: np<span style=color:#f92672>.</span>ndarray, prior_cov: np<span style=color:#f92672>.</span>ndarray, noise_var: float):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>prior_mean <span style=color:#f92672>=</span> prior_mean[:, np<span style=color:#f92672>.</span>newaxis] <span style=color:#75715e># column vector of shape (1, d)</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>prior_cov <span style=color:#f92672>=</span> prior_cov <span style=color:#75715e># matrix of shape (d, d)</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># We initalize the prior distribution over the parameters using the given mean and covariance matrix</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># In the formulas above this corresponds to m_0 (prior_mean) and S_0 (prior_cov)</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>prior <span style=color:#f92672>=</span> multivariate_normal(prior_mean, prior_cov)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># We also know the variance of the noise</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>noise_var <span style=color:#f92672>=</span> noise_var <span style=color:#75715e># single float value</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>noise_precision <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> noise_var
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Before performing any inference the parameter posterior equals the parameter prior</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>param_posterior <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>prior
</span></span><span style=display:flex><span>        <span style=color:#75715e># Accordingly, the posterior mean and covariance equal the prior mean and variance</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>post_mean <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>prior_mean <span style=color:#75715e># corresponds to m_N in formulas</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>post_cov <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>prior_cov <span style=color:#75715e># corresponds to S_N in formulas</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span><span style=color:#75715e># Let&#39;s make sure that we can initialize our model</span>
</span></span><span style=display:flex><span>prior_mean <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>prior_cov <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>0</span>], [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0.5</span>]])
</span></span><span style=display:flex><span>noise_var <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.2</span>
</span></span><span style=display:flex><span>blr <span style=color:#f92672>=</span> BayesianLinearRegression(prior_mean, prior_cov, noise_var)
</span></span></code></pre></div><h3 id=62-generating-a-dataset>6.2 Generating a dataset</h3><p>Before going any further we need a dataset to test our implementation. Remember that we assume that our targets were generated by a function of the form $y = \pmb{\phi}^T(\pmb{x}) \pmb{\theta} + \epsilon$ where $\epsilon$ is normally distributed with mean $0$ and some known variance $\sigma^2$: $\epsilon \sim \mathcal{N}(0, \sigma^2)$.</p><p>To keep things simple we will work with one-dimensional data and simple linear regression (that is, no non-linear transformation of the inputs). Consequently, our data generating function will be of the form
$$ y = \theta_0 + \theta_1 , x + \epsilon $$</p><p>Note that we added a parameter $\theta_0$ which corresponds to the intercept of the linear function. Until know we assumed $\theta_0 = 0$. As mentioned earlier, $\theta_1$ represents the slope of the linear function.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>%</span>matplotlib inline
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_function_labels</span>(slope: float, intercept: float, noise_std_dev: float, data: np<span style=color:#f92672>.</span>ndarray) <span style=color:#f92672>-&gt;</span> np<span style=color:#f92672>.</span>ndarray:
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Compute target values given function parameters and data.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        slope: slope of the function (theta_1)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        intercept: intercept of the function (theta_0)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        data: input feature values (x)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        noise_std_dev: standard deviation of noise distribution (sigma)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        target values, either true or corrupted with noise
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    n_samples <span style=color:#f92672>=</span> len(data)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> noise_std_dev <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>: <span style=color:#75715e># Real function</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> slope <span style=color:#f92672>*</span> data <span style=color:#f92672>+</span> intercept
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>: <span style=color:#75715e># Noise corrupted</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> slope <span style=color:#f92672>*</span> data <span style=color:#f92672>+</span> intercept <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(<span style=color:#ae81ff>0</span>, noise_std_dev, n_samples)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Set random seed to ensure reproducibility</span>
</span></span><span style=display:flex><span>seed <span style=color:#f92672>=</span> <span style=color:#ae81ff>42</span>
</span></span><span style=display:flex><span>np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>seed(seed)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Generate true values and noise corrupted targets</span>
</span></span><span style=display:flex><span>n_datapoints <span style=color:#f92672>=</span> <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>intercept <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>0.7</span>
</span></span><span style=display:flex><span>slope <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.9</span>
</span></span><span style=display:flex><span>noise_std_dev <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>noise_var <span style=color:#f92672>=</span> noise_std_dev<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>lower_bound <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1.5</span>
</span></span><span style=display:flex><span>upper_bound <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.5</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Generate dataset</span>
</span></span><span style=display:flex><span>features <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>uniform(lower_bound, upper_bound, n_datapoints)
</span></span><span style=display:flex><span>labels <span style=color:#f92672>=</span> compute_function_labels(slope, intercept, <span style=color:#ae81ff>0.</span>, features)
</span></span><span style=display:flex><span>noise_corrupted_labels <span style=color:#f92672>=</span> compute_function_labels(slope, intercept, noise_std_dev, features)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Plot the dataset</span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>7</span>))
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(features, labels, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;r&#39;</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;True values&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(features, noise_corrupted_labels, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Noise corrupted values&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#34;Features&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#34;Labels&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;Real function along with noisy targets&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend();
</span></span></code></pre></div><img src=/posts/machine_learning/images/real_function_with_noisy_targets.png width=80% class=center><h3 id=63-step-2-posterior-over-the-parameters>6.3 Step 2: Posterior over the parameters</h3><p>We finished setting up our probabilistic model. Next, we want to use this model and our dataset $\mathcal{X, Y}$ to estimate the parameter posterior $p(\pmb{\theta} | \mathcal{X, Y})$. Keep in mind that we don&rsquo;t compute point estimates of the parameters. Instead, we determine the mean and variance of the (Gaussian) posterior distribution and use this entire distribution when making predictions.</p><p>We can estimate the parameter posterior using Bayes theorem:
$$
p(\pmb{\theta} \mid \mathcal{X}, \mathcal{Y})=\frac{p(\mathcal{Y} \mid \mathcal{X}, \pmb{\theta}) p(\pmb{\theta})}{p(\mathcal{Y} \mid \mathcal{X})}
$$</p><ul><li>$p(\mathcal{Y} \mid \mathcal{X}, \pmb{\theta})$ is the likelihood function, $p(\mathcal{Y} \mid \mathcal{X}, \pmb{\theta})=\mathcal{N}\left(\pmb{y} \mid \pmb{\Phi} \pmb{\theta}, \sigma^{2} \pmb{I}\right)$</li><li>$p(\pmb{\theta})$ is the prior distribution, $p(\pmb{\theta})=\mathcal{N}\left(\pmb{\theta} \mid \pmb{m}_{0}, \pmb{S}_{0}\right)$</li><li>$p(\mathcal{Y} \mid \mathcal{X})=\int p(\mathcal{Y} \mid \mathcal{X}, \pmb{\theta}) p(\pmb{\theta}) \mathrm{d} \pmb{\theta}$ is the evidence which ensures that the posterior is normalized (that is, that it integrates to 1).</li></ul><p>The parameter posterior can be estimated in closed form (for proof see theorem 9.1 in the book <a href=https://mml-book.com target=_blank rel=noopener>Mathematics for Machine Learning</a>):
$$
\begin{aligned}
p(\pmb{\theta} \mid \mathcal{X}, \mathcal{Y}) &=\mathcal{N}\left(\pmb{\theta} \mid \pmb{m}_{N}, \pmb{S}_{N}\right) \\\
\pmb{S}_{N} &=\left(\pmb{S}_{0}^{-1}+\sigma^{-2} \pmb{\Phi}^{\top} \pmb{\Phi}\right)^{-1} \\\
\pmb{m}_{N} &=\pmb{S}_{N}\left(\pmb{S}_{0}^{-1} \pmb{m}_{0}+\sigma^{-2} \pmb{\Phi}^{\top} \pmb{y}\right)
\end{aligned}
$$</p><p>Coming back to our <code>BayesianLinearRegression</code> class we need to add a method which allows us to update the posterior distribution given a dataset.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> scipy.stats <span style=color:#f92672>import</span> multivariate_normal
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> scipy.stats <span style=color:#f92672>import</span> norm <span style=color:#66d9ef>as</span> univariate_normal
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>BayesianLinearRegression</span>:
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34; Bayesian linear regression
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        prior_mean: Mean values of the prior distribution (m_0)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        prior_cov: Covariance matrix of the prior distribution (S_0)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        noise_var: Variance of the noise distribution
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, prior_mean: np<span style=color:#f92672>.</span>ndarray, prior_cov: np<span style=color:#f92672>.</span>ndarray, noise_var: float):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>prior_mean <span style=color:#f92672>=</span> prior_mean[:, np<span style=color:#f92672>.</span>newaxis] <span style=color:#75715e># column vector of shape (1, d)</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>prior_cov <span style=color:#f92672>=</span> prior_cov <span style=color:#75715e># matrix of shape (d, d)</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># We initalize the prior distribution over the parameters using the given mean and covariance matrix</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># In the formulas above this corresponds to m_0 (prior_mean) and S_0 (prior_cov)</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>prior <span style=color:#f92672>=</span> multivariate_normal(prior_mean, prior_cov)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># We also know the variance of the noise</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>noise_var <span style=color:#f92672>=</span> noise_var <span style=color:#75715e># single float value</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>noise_precision <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> noise_var
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Before performing any inference the parameter posterior equals the parameter prior</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>param_posterior <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>prior
</span></span><span style=display:flex><span>        <span style=color:#75715e># Accordingly, the posterior mean and covariance equal the prior mean and variance</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>post_mean <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>prior_mean <span style=color:#75715e># corresponds to m_N in formulas</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>post_cov <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>prior_cov <span style=color:#75715e># corresponds to S_N in formulas</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>update_posterior</span>(self, features: np<span style=color:#f92672>.</span>ndarray, targets: np<span style=color:#f92672>.</span>ndarray):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Update the posterior distribution given new features and targets
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            features: numpy array of features
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            targets: numpy array of targets
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Reshape targets to allow correct matrix multiplication</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Input shape is (N,) but we need (N, 1)</span>
</span></span><span style=display:flex><span>        targets <span style=color:#f92672>=</span> targets[:, np<span style=color:#f92672>.</span>newaxis]
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Compute the design matrix, shape (N, 2)</span>
</span></span><span style=display:flex><span>        design_matrix <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>compute_design_matrix(features)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Update the covariance matrix, shape (2, 2)</span>
</span></span><span style=display:flex><span>        design_matrix_dot_product <span style=color:#f92672>=</span> design_matrix<span style=color:#f92672>.</span>T<span style=color:#f92672>.</span>dot(design_matrix)
</span></span><span style=display:flex><span>        inv_prior_cov <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>inv(self<span style=color:#f92672>.</span>prior_cov)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>post_cov <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>inv(inv_prior_cov <span style=color:#f92672>+</span>  self<span style=color:#f92672>.</span>noise_precision <span style=color:#f92672>*</span> design_matrix_dot_product)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Update the mean, shape (2, 1)</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>post_mean <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>post_cov<span style=color:#f92672>.</span>dot( 
</span></span><span style=display:flex><span>                         inv_prior_cov<span style=color:#f92672>.</span>dot(self<span style=color:#f92672>.</span>prior_mean) <span style=color:#f92672>+</span> 
</span></span><span style=display:flex><span>                         self<span style=color:#f92672>.</span>noise_precision <span style=color:#f92672>*</span> design_matrix<span style=color:#f92672>.</span>T<span style=color:#f92672>.</span>dot(targets))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Update the posterior distribution</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>param_posterior <span style=color:#f92672>=</span> multivariate_normal(self<span style=color:#f92672>.</span>post_mean<span style=color:#f92672>.</span>flatten(), self<span style=color:#f92672>.</span>post_cov)
</span></span><span style=display:flex><span>                
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_design_matrix</span>(self, features: np<span style=color:#f92672>.</span>ndarray) <span style=color:#f92672>-&gt;</span> np<span style=color:#f92672>.</span>ndarray:
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Compute the design matrix. To keep things simple we use simple linear
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        regression and add the value phi_0 = 1 to our input data.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            features: numpy array of features
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            design_matrix: numpy array of transformed features
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &gt;&gt;&gt; compute_design_matrix(np.array([2, 3]))
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        np.array([[1., 2.], [1., 3.])
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        n_samples <span style=color:#f92672>=</span> len(features)
</span></span><span style=display:flex><span>        phi_0 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>ones(n_samples)
</span></span><span style=display:flex><span>        design_matrix <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>stack((phi_0, features), axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> design_matrix
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict</span>(self, features: np<span style=color:#f92672>.</span>ndarray):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Compute predictive posterior given new datapoint
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            features: 1d numpy array of features
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            pred_posterior: predictive posterior distribution
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        design_matrix <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>compute_design_matrix(features)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        pred_mean <span style=color:#f92672>=</span> design_matrix<span style=color:#f92672>.</span>dot(self<span style=color:#f92672>.</span>post_mean)
</span></span><span style=display:flex><span>        pred_cov <span style=color:#f92672>=</span> design_matrix<span style=color:#f92672>.</span>dot(self<span style=color:#f92672>.</span>post_cov<span style=color:#f92672>.</span>dot(design_matrix<span style=color:#f92672>.</span>T)) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>noise_var
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        pred_posterior <span style=color:#f92672>=</span> univariate_normal(loc<span style=color:#f92672>=</span>pred_mean<span style=color:#f92672>.</span>flatten(), scale<span style=color:#f92672>=</span>pred_cov<span style=color:#f92672>**</span><span style=color:#ae81ff>0.5</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> pred_posterior
</span></span></code></pre></div><h3 id=64-visualizing-the-parameter-posterior>6.4 Visualizing the parameter posterior</h3><p>To ensure that our implementation is correct we can visualize how the posterior over the parameters changes as the model sees more data. We will visualize the distribution using a <a href=https://en.wikipedia.org/wiki/Contour_line target=_blank rel=noopener>contour plot</a> - a method for visualizing three-dimensional functions. In our case we want to visualize the density of our bi-variate Gaussian for each point (that is, each slope/intercept combination). The plot below shows an example which illustrates how the lines and colours of a contour plot correspond to a Gaussian distribution:</p><img src=/posts/machine_learning/images/density_plot.png width=50% class=center><p>As we can see, the density is highest in the yellow regions decreasing when moving further out into the green and blue parts. This should give you a better understanding of contour plots.</p><p>To analyze our Bayesian linear regression class we will start by initializing a new model. We can visualize its prior distribution over the parameters <em>before</em> the model has seen any real data.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Initialize BLR model</span>
</span></span><span style=display:flex><span>prior_mean <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>prior_cov <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span><span style=color:#f92672>/</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>identity(<span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>blr <span style=color:#f92672>=</span> BayesianLinearRegression(prior_mean, prior_cov, noise_var)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>plot_param_posterior</span>(lower_bound, upper_bound, blr, title):
</span></span><span style=display:flex><span>    fig <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>figure()
</span></span><span style=display:flex><span>    mesh_features, mesh_labels <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mgrid[lower_bound:upper_bound:<span style=color:#ae81ff>.01</span>, lower_bound:upper_bound:<span style=color:#ae81ff>.01</span>]
</span></span><span style=display:flex><span>    pos <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dstack((mesh_features, mesh_labels))
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>contourf(mesh_features, mesh_labels, blr<span style=color:#f92672>.</span>param_posterior<span style=color:#f92672>.</span>pdf(pos), levels<span style=color:#f92672>=</span><span style=color:#ae81ff>15</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>scatter(intercept, slope, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;red&#39;</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;True parameter values&#34;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>title(title)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#34;Intercept&#34;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#34;Slope&#34;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>legend();
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#75715e># Visualize parameter prior distribution</span>
</span></span><span style=display:flex><span>plot_param_posterior(lower_bound, upper_bound, blr, title<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Prior parameter distribution&#34;</span>)
</span></span></code></pre></div><img src=/posts/machine_learning/images/prior_parameter_distribution.png width=50% class=center><p>The plot above illustrates both the prior parameter distribution and the true parameter values that we want to find. If our model works correctly, the posterior distribution should become more narrow and move closer to the true parameter values as the model sees more datapoints. This can be visualized with contour plots, too! Below we update the posterior distribution iteratively as the model sees more and more data. The contour plots for each step show how the parameter posterior develops and converges close to the true values in the end.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>n_points_lst <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>50</span>, <span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>200</span>, <span style=color:#ae81ff>500</span>, <span style=color:#ae81ff>1000</span>]
</span></span><span style=display:flex><span>previous_n_points <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> n_points <span style=color:#f92672>in</span> n_points_lst:
</span></span><span style=display:flex><span>    train_features <span style=color:#f92672>=</span> features[previous_n_points:n_points]
</span></span><span style=display:flex><span>    train_labels <span style=color:#f92672>=</span> noise_corrupted_labels[previous_n_points:n_points]
</span></span><span style=display:flex><span>    blr<span style=color:#f92672>.</span>update_posterior(train_features, train_labels)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Visualize updated parameter posterior distribution</span>
</span></span><span style=display:flex><span>    plot_param_posterior(lower_bound, 
</span></span><span style=display:flex><span>                         upper_bound, 
</span></span><span style=display:flex><span>                         blr, 
</span></span><span style=display:flex><span>                         title<span style=color:#f92672>=</span><span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Updated parameter distribution using </span><span style=color:#e6db74>{</span>n_points<span style=color:#e6db74>}</span><span style=color:#e6db74> datapoints&#34;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    previous_n_points <span style=color:#f92672>=</span> n_points
</span></span></code></pre></div><p><img src=/posts/machine_learning/images/param_post_1_datapoint.png width=50% class=center>
<img src=/posts/machine_learning/images/param_post_10_datapoints.png width=50% class=center>
<img src=/posts/machine_learning/images/param_post_50_datapoints.png width=50% class=center>
<img src=/posts/machine_learning/images/param_post_100_datapoints.png width=50% class=center>
<img src=/posts/machine_learning/images/param_post_200_datapoints.png width=50% class=center>
<img src=/posts/machine_learning/images/param_post_500_datapoints.png width=50% class=center>
<img src=/posts/machine_learning/images/param_post_1000_datapoints.png width=50% class=center></p><h3 id=65-step-3-posterior-predictive-distribution>6.5 Step 3: Posterior predictive distribution</h3><p>Given the posterior distribution over the parameters we can determine the predictive distribution (= posterior over the outputs) for a new input $(\pmb{x}_*, y_*)$. This is the distribution we are really interested in. A trained model is not particularly useful when we can&rsquo;t use it to make predictions, right?</p><p>The posterior predictive distribution looks as follows:</p><p>$$
\begin{aligned}
p\left(y_{*} \mid \mathcal{X}, \mathcal{Y}, \pmb{x}_*\right) &=\int p\left(y_{*} \mid \pmb{x}_{*}, \pmb{\theta}\right) p(\pmb{\theta} \mid \mathcal{X}, \mathcal{Y}) \mathrm{d} \pmb{\theta} \\\
&=\int \mathcal{N}\left(y_{*} \mid \pmb{\phi}^{\top}\left(\pmb{x}_{*}\right) \pmb{\theta}, \sigma^{2}\right) \mathcal{N}\left(\pmb{\theta} \mid \pmb{m}_{N}, \pmb{S}_{N}\right) \mathrm{d} \pmb{\theta} \\\
&=\mathcal{N}\left(y_{*} \mid \pmb{\phi}^{\top}\left(\pmb{x}_{*}\right) \pmb{m}_{N}, \pmb{\phi}^{\top}\left(\pmb{x}_{*}\right) \pmb{S}_{N} \pmb{\phi}\left(\pmb{x}_{*}\right)+\sigma^{2}\right)
\end{aligned}
$$</p><p>First of all: note that the predictive posterior for a new input $\pmb{x}_{*}$ is a <em>univariate</em> Gaussian distribution. We can see that the mean of the distribution is given by the product of the design matrix for the new example ($\pmb{\phi}^{\top}\left(\pmb{x}_{*}\right)$) and the mean of the parameter posterior ($\pmb{m}_{N}$). The variance $(\pmb{\phi}^{\top}\left(\pmb{x}_{*}\right) \pmb{S}_{N} \pmb{\phi}\left(\pmb{x}_{*}\right)+\sigma^{2}$) of the predictive posterior has two parts:</p><ol><li>$\sigma^{2}$: The variance of the noise</li><li>$\pmb{\phi}^{\top}\left(\pmb{x}_{*}\right) \pmb{S}_{N} \pmb{\phi}\left(\pmb{x}_{*}\right)$: The posterior uncertainty associated with the parameters $\pmb{\theta}$</li></ol><p>Let&rsquo;s add a <code>predict</code> method to our <code>BayesianLinearRegression</code> class which computes the predictive posterior for a new input (you will find the method in the class definition above):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict</span>(self, features: np<span style=color:#f92672>.</span>ndarray):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Compute predictive posterior given new datapoint
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        features: 1d numpy array of features
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        pred_posterior: predictive posterior distribution
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    design_matrix <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>compute_design_matrix(features)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    pred_mean <span style=color:#f92672>=</span> design_matrix<span style=color:#f92672>.</span>dot(self<span style=color:#f92672>.</span>post_mean)
</span></span><span style=display:flex><span>    pred_cov <span style=color:#f92672>=</span> design_matrix<span style=color:#f92672>.</span>dot(self<span style=color:#f92672>.</span>post_cov<span style=color:#f92672>.</span>dot(design_matrix<span style=color:#f92672>.</span>T)) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>noise_var
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    pred_posterior <span style=color:#f92672>=</span> univariate_normal(pred_mean<span style=color:#f92672>.</span>flatten(), pred_cov)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> pred_posterior
</span></span></code></pre></div><h3 id=66-visualizing-the-predictive-posterior--a-classanchor-idpredictive-posterior-visualizationa>6.6 Visualizing the predictive posterior</h3><p>Our original dataset follows a simple linear function. After training the model it should be able to predict labels for new datapoints, even if they lie beyond the range from [-1.5, 1.5]. But how can we get from the predictive distribution that our model computes to actual labels? That&rsquo;s easy: we <em>sample</em> from the predictive posterior.</p><p>To make sure that we are all on the same page: given a new input example our Bayesian linear regression model predicts not a single label but a <em>distribution</em> over possible labels. This distribution is Gaussian. We can get actual labels by sampling from this distribution.</p><p>The code below implements and visualizes this:</p><ul><li>We create some test features for which we want predictions</li><li>Each feature is given to the trained BLR model which returns a univariate Gaussian distribution over possible labels (<code>pred_posterior = blr.predict(np.array([feat]))</code>)</li><li>We sample from this distribution (<code>sample_predicted_labels = pred_posterior.rvs(size=sample_size)</code>)</li><li>The predicted labels are saved in a format that makes it easy to plot them</li><li>Finally, we plot each input feature, its true label and the sampled predictions. Remember: the samples are generated from the predictive posterior returned by the <code>predict</code> method. Think of a Gaussian distribution plotted along the y-axis for each feature. We visualize this with a histogram: more likely values close to the mean will be sampled more often than less likely values.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> seaborn <span style=color:#66d9ef>as</span> sns
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>all_rows <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>sample_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>test_features <span style=color:#f92672>=</span> [<span style=color:#f92672>-</span><span style=color:#ae81ff>2</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>]
</span></span><span style=display:flex><span>all_labels <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> feat <span style=color:#f92672>in</span> test_features:
</span></span><span style=display:flex><span>    true_label <span style=color:#f92672>=</span> compute_function_labels(slope, intercept, <span style=color:#ae81ff>0</span>, np<span style=color:#f92672>.</span>array([feat]))
</span></span><span style=display:flex><span>    all_labels<span style=color:#f92672>.</span>append(true_label)
</span></span><span style=display:flex><span>    pred_posterior <span style=color:#f92672>=</span> blr<span style=color:#f92672>.</span>predict(np<span style=color:#f92672>.</span>array([feat]))
</span></span><span style=display:flex><span>    sample_predicted_labels <span style=color:#f92672>=</span> pred_posterior<span style=color:#f92672>.</span>rvs(size<span style=color:#f92672>=</span>sample_size)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> label <span style=color:#f92672>in</span> sample_predicted_labels:
</span></span><span style=display:flex><span>        all_rows<span style=color:#f92672>.</span>append([feat, label])
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>all_data <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame(all_rows, columns<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;feature&#34;</span>, <span style=color:#e6db74>&#34;label&#34;</span>]) 
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>displot(data<span style=color:#f92672>=</span>all_data, x<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;feature&#34;</span>, y<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;label&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(x<span style=color:#f92672>=</span>test_features, y<span style=color:#f92672>=</span>all_labels, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;red&#34;</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;True values&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;Predictive posterior distributions&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot();
</span></span></code></pre></div><img src=/posts/machine_learning/images/predictive_posterior.png width=40% class=center><h2 id=7-sources-and-further-reading>7. Sources and further reading</h2><p>The basis for this notebook is chapter 9.2 of the book <a href=https://mml-book.com target=_blank rel=noopener>Mathematics for Machine Learning</a>. I can highly recommend to read through chapter 9 to get a deeper understanding of (Bayesian) linear regression.</p><p>You will find explanations and an implementation of simple linear regression in the <a href=https://github.com/zotroneneis/machine_learning_basics/blob/master/linear_regression.ipynb target=_blank rel=noopener>notebook on linear regression</a></p></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/zotroneneis/zotroneneis.github.io/edit/main/content/posts/machine_learning/bayesian_linear_regression.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
Improve this page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/books/deep_work/ title="Deep Work" class="btn filled-button"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>Deep Work</div></a></div><div class="col-md-6 next-article"><a href=/posts/machine_learning/bayesian_linear_regression_2/ title="Bayesian linear regression 2" class="btn filled-button"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>Bayesian linear regression 2</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn data-toggle=tooltip data-placement=left title="Scroll to top"><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#1-what-is-bayesian-linear-regression-blr>1. What is Bayesian linear regression (BLR)?</a></li><li><a href=#2-recap-linear-regression>2. Recap linear regression</a></li><li><a href=#3-fundamental-concepts>3. Fundamental concepts</a></li><li><a href=#4-linear-regression-from-a-probabilistic-perspective>4. Linear regression from a probabilistic perspective</a></li><li><a href=#5-linear-regression-with-basis-functions>5. Linear regression with basis functions</a><ul><li><a href=#51-example-basis-functions>5.1 Example basis functions</a><ul><li><a href=#linear-regression>Linear regression</a></li><li><a href=#polynomial-regression>Polynomial regression</a></li></ul></li><li><a href=#52-the-design-matrix>5.2 The design matrix</a></li></ul></li><li><a href=#6-bayesian-linear-regression>6. Bayesian linear regression</a><ul><li><a href=#61-step-1-probabilistic-model>6.1 Step 1: Probabilistic model</a></li><li><a href=#62-generating-a-dataset>6.2 Generating a dataset</a></li><li><a href=#63-step-2-posterior-over-the-parameters>6.3 Step 2: Posterior over the parameters</a></li><li><a href=#64-visualizing-the-parameter-posterior>6.4 Visualizing the parameter posterior</a></li><li><a href=#65-step-3-posterior-predictive-distribution>6.5 Step 3: Posterior predictive distribution</a></li><li><a href=#66-visualizing-the-predictive-posterior--a-classanchor-idpredictive-posterior-visualizationa>6.6 Visualizing the predictive posterior</a></li></ul></li><li><a href=#7-sources-and-further-reading>7. Sources and further reading</a></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://alpopkes.com/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://alpopkes.com/#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=https://alpopkes.com/#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=https://alpopkes.com/#education>Education</a></li><li class=nav-item><a class=smooth-scroll href=https://alpopkes.com/#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=https://alpopkes.com/#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=https://alpopkes.com/#appearances>Talks & Podcasts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:popkes@gmx.net target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>popkes@gmx.net</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hugo-toha/toha target=_blank rel=noopener><img src=/images/theme-logo_hu16779671404603505019.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020-2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/application.a01d837827c5183eb1439ce1afe5db5896d8b892671bcbe43996542de67b6510.js integrity="sha256-oB2DeCfFGD6xQ5zhr+XbWJbYuJJnG8vkOZZULeZ7ZRA=" defer></script></body></html>