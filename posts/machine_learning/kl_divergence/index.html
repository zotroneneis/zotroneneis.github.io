<!doctype html><html lang=en><head><title>Kullback-Leibler Divergence</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/application.c27cd4e332ca297fb25625ebfb0fd08824b3344ee00869c5bc395be617ba225b.css integrity="sha256-wnzU4zLKKX+yViXr+w/QiCSzNE7gCGnFvDlb5he6Ils="><link rel=icon type=image/png href=/images/author/image_al_hu15845133475761513827.jpg><meta property="og:url" content="https://alpopkes.com/posts/machine_learning/kl_divergence/"><meta property="og:site_name" content="Anna-Lena Popkes"><meta property="og:title" content="Kullback-Leibler Divergence"><meta property="og:description" content="One of the points on my long ‘stuff-you-have-to-look-at’ list is the Kullback-Leibler divergence. I finally took the time to take a detailed look at this topic.
Definition The KL-divergence is a measure of how similar (or different) two probablity distributions are. When having a discrete probability distribution $P$ and another probability distribution $Q$ the KL-divergence for a set of points $X$ is defined as:
$$D_{KL}(P ,|| ,Q) = \sum_{x \in X} P(x) \log \big( \frac{P(x)}{Q(x)} \big)$$"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-02-02T00:00:00+00:00"><meta property="article:modified_time" content="2019-02-02T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Kullback-Leibler Divergence"><meta name=twitter:description content="One of the points on my long ‘stuff-you-have-to-look-at’ list is the Kullback-Leibler divergence. I finally took the time to take a detailed look at this topic.
Definition The KL-divergence is a measure of how similar (or different) two probablity distributions are. When having a discrete probability distribution $P$ and another probability distribution $Q$ the KL-divergence for a set of points $X$ is defined as:
$$D_{KL}(P ,|| ,Q) = \sum_{x \in X} P(x) \log \big( \frac{P(x)}{Q(x)} \big)$$"><meta name=description content="Kullback-Leibler Divergence"><script>theme=localStorage.getItem("darkmode:color-scheme")||"system",theme=="system"&&(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?theme="dark":theme="light"),document.documentElement.setAttribute("data-theme",theme)</script></head><body class="type-posts kind-page" data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-secondary wrapper"><nav class="navbar navbar-expand-xl top-navbar shadow" id=top-navbar><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button>
<i data-feather=sidebar></i>
</button>
<a class=navbar-brand href=/><img src=/images/author/image_al_hu15845133475761513827.jpg id=logo alt=Logo>
Anna-Lena Popkes</a>
<button class="navbar-toggler navbar-light" id=navbar-toggler type=button data-toggle=collapse data-target=#top-nav-items aria-label=menu>
<i data-feather=menu></i></button><div class="collapse navbar-collapse dynamic-navbar" id=top-nav-items><ul class="nav navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/#home>Home</a></li><li class=nav-item><a class=nav-link href=/#about>About</a></li><li class=nav-item><a class=nav-link href=/#skills>Skills</a></li><li class=nav-item><a class=nav-link href=/#experiences>Experiences</a></li><li class=nav-item><a class=nav-link href=/#education>Education</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>More</a><div class=dropdown-menu aria-labelledby=navbarDropdown><a class=dropdown-item href=/#projects>Projects</a>
<a class=dropdown-item href=/#appearances>Talks & Podcasts</a>
<a class=dropdown-item href=/#recent-posts>Recent Posts</a></div></li><div id=top-navbar-divider></div><li class=nav-item><a class=nav-link id=blog-link href=/posts>Posts</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg class=theme-icon src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme"></a><div id=themeMenu class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# data-scheme=light><img class=theme-icon src=/icons/sun-svgrepo-com.svg width=20 alt="Light Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=dark><img class=theme-icon src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=system><img class=theme-icon src=/icons/computer-svgrepo-com.svg width=20 alt="System Theme"></a></div></li></ul></div></div><img src=/images/author/image_al_hu15845133475761513827.jpg class=d-none id=main-logo alt=Logo>
<img src=/images/author/image_al_hu15845133475761513827.jpg class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts/ data-filter=all>Posts</a></li><div class=subtree><li><a class=list-link href=/posts/news/ title="Personal news">Personal news</a></li><li><a class=list-link href=/posts/my_path_to_ml/ title="My path to machine learning">My path to machine learning</a></li><li><i data-feather=plus-circle></i><a class=list-link href=/posts/books/> Books</a><ul><li><a class=list-link href=/posts/books/reading_list/ title="Personal reading List">Personal reading List</a></li><li><a class=list-link href=/posts/books/deep_work/ title="Deep work">Deep work</a></li></ul></li><li><i data-feather=minus-circle></i><a class="active list-link" href=/posts/machine_learning/> Machine Learning</a><ul class=active><li><a class=list-link href=/posts/machine_learning/bayesian_linear_regression/ title="Bayesian linear regression">Bayesian linear regression</a></li><li><a class=list-link href=/posts/machine_learning/bayesian_linear_regression_2/ title="Bayesian linear regression 2">Bayesian linear regression 2</a></li><li><a class="active list-link" href=/posts/machine_learning/kl_divergence/ title="KL Divergence">KL Divergence</a></li><li><a class=list-link href=/posts/machine_learning/principal_component_analysis/ title="Principal component analysis (PCA)">Principal component analysis (PCA)</a></li><li><a class=list-link href=/posts/machine_learning/support_vector_machines/ title="Support vector machines">Support vector machines</a></li><li><a class=list-link href=/posts/machine_learning/variational_inference/ title="Variational Inference">Variational Inference</a></li></ul></li><li><i data-feather=plus-circle></i><a class=list-link href=/posts/python/> Python</a><ul><li><a class=list-link href=/posts/python/coding_with_kids/ title="Coding with kids">Coding with kids</a></li><li><a class=list-link href=/posts/python/mocking/ title=Mocking>Mocking</a></li><li><a class=list-link href=/posts/python/packaging_tools/ title="Packaging tools">Packaging tools</a></li><li><i data-feather=plus-circle></i><a class=list-link href=/posts/python/magical_universe/> Magical Universe</a><ul><li><a class=list-link href=/posts/python/magical_universe/day_1_start/ title=Start>Start</a></li><li><a class=list-link href=/posts/python/magical_universe/day_1_magical_universe/ title="The Tales of Castle Kilmere">The Tales of Castle Kilmere</a></li><li><a class=list-link href=/posts/python/magical_universe/day_1_first_post_oop/ title="Object-oriented programming">Object-oriented programming</a></li><li><a class=list-link href=/posts/python/magical_universe/day_2_types_of_methods/ title="Types of methods">Types of methods</a></li><li><a class=list-link href=/posts/python/magical_universe/day_3_type_annotations/ title="Type annotations">Type annotations</a></li><li><a class=list-link href=/posts/python/magical_universe/day_4_to_string_conversion/ title="To-string conversion">To-string conversion</a></li><li><a class=list-link href=/posts/python/magical_universe/day_5_decorators/ title=Decorators>Decorators</a></li><li><a class=list-link href=/posts/python/magical_universe/day_6_properties/ title=Properties>Properties</a></li><li><a class=list-link href=/posts/python/magical_universe/day_7_underscore_patterns/ title="Underscore patterns">Underscore patterns</a></li><li><a class=list-link href=/posts/python/magical_universe/day_8_extending_universe/ title="Extending the universe">Extending the universe</a></li><li><a class=list-link href=/posts/python/magical_universe/day_9_duck_typing/ title="Duck Typing">Duck Typing</a></li><li><a class=list-link href=/posts/python/magical_universe/day_10_11_namedtuples/ title=Namedtuples>Namedtuples</a></li><li><a class=list-link href=/posts/python/magical_universe/day_12_to_15_abcs/ title="Abstract Base Classes">Abstract Base Classes</a></li><li><a class=list-link href=/posts/python/magical_universe/day_16_to_18_data_classes/ title="Data classes">Data classes</a></li><li><a class=list-link href=/posts/python/magical_universe/day_19_immutable_data_classes/ title="Immutable data classes">Immutable data classes</a></li><li><a class=list-link href=/posts/python/magical_universe/day_20_decorators_in_classes/ title="Decorators in classes">Decorators in classes</a></li><li><a class=list-link href=/posts/python/magical_universe/day_21_if_main/ title='if __name__ == "__main__"'>if __name__ == "__main__"</a></li><li><a class=list-link href=/posts/python/magical_universe/day_22_to_24_context_managers/ title="Context managers">Context managers</a></li><li><a class=list-link href=/posts/python/magical_universe/day_25_to_28_pytest/ title="Testing with pytest">Testing with pytest</a></li><li><a class=list-link href=/posts/python/magical_universe/day_29_to_31_iterators/ title=Iterators>Iterators</a></li><li><a class=list-link href=/posts/python/magical_universe/day_34_multisets/ title=Multisets>Multisets</a></li><li><a class=list-link href=/posts/python/magical_universe/day_37_extending_universe/ title="Extending the universe II">Extending the universe II</a></li><li><a class=list-link href=/posts/python/magical_universe/day_43_to_45_exception_classes/ title="Exception classes">Exception classes</a></li><li><a class=list-link href=/posts/python/magical_universe/day_46_functools_wraps/ title=functools.wraps>functools.wraps</a></li><li><a class=list-link href=/posts/python/magical_universe/day_47_to_48_defaultdict/ title=Defaultdict>Defaultdict</a></li><li><a class=list-link href=/posts/python/magical_universe/day_49_to_50_config_files/ title="Config files">Config files</a></li><li><a class=list-link href=/posts/python/magical_universe/2018-09-16-blog-post-day-51/ title="Wrap up">Wrap up</a></li></ul></li></ul></li><li><i data-feather=plus-circle></i><a class=list-link href=/posts/software_engineering/> Software Engineering</a><ul><li><a class=list-link href=/posts/software_engineering/containers/ title="Intro to containers">Intro to containers</a></li><li><a class=list-link href=/posts/software_engineering/docker/ title="Intro to Docker">Intro to Docker</a></li><li><a class=list-link href=/posts/software_engineering/virtual_machines/ title="Intro to virtual machines">Intro to virtual machines</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/image_al_hu6187147171234605626.jpg alt="Author Image"><h5 class=author-name>Anna-Lena Popkes</h5><p class=text-muted>Saturday, February 2, 2019</p></div><div class=title><h1>Kullback-Leibler Divergence</h1></div><div class=post-content id=post-content><p>One of the points on my long &lsquo;stuff-you-have-to-look-at&rsquo; list is the Kullback-Leibler divergence. I finally took the time to take a detailed look at this topic.</p><h2 id=definition>Definition</h2><p>The KL-divergence is a measure of how similar (or different) two probablity distributions are. When having a discrete probability distribution $P$ and another probability distribution $Q$ the KL-divergence for a set of points $X$ is defined as:</p><p>$$D_{KL}(P ,|| ,Q) = \sum_{x \in X} P(x) \log \big( \frac{P(x)}{Q(x)} \big)$$</p><p>For probability distributions over continuous variables the sum turns into an integral:</p><p>$$ D_{KL}(P ,|| ,Q) = \int_{-\infty}^{\infty} p(x) \log \big( \frac{p(x)}{q(x)} \big) dx$$</p><p>where $p$ and $q$ denote the probability density functions of $P$ and $Q$.</p><p><a href=https://en.wikipedia.org/wiki/Kullback_Leibler_divergence target=_blank rel=noopener>Source</a></p><h2 id=visual-example-a-classanchor-idvisual-examplea>Visual example</h2><p>The <a href=https://en.wikipedia.org/wiki/Kullback%e2%80%93Leibler_divergence target=_blank rel=noopener>Wikipedia entry</a> on the KL divergence contains a nice illustration:</p><img src=/posts/machine_learning/images/kl_divergence.png width=80% class=center><p>On the left hand side we can see two Gaussian probability density functions $p(x)$ and $q(x)$. The right hand side show the area that is integrated when computing the KL divergence from $p$ to $q$. We know that:</p><p>$$
\begin{align}
D_{KL}(P ,|| ,Q) &= \int_{-\infty}^{\infty} p(x) \log \big( \frac{p(x)}{q(x)} \big) dx \
&= \int_{-\infty}^{\infty} p(x) \big( \log p(x) - \log q(x) \big) dx
\end{align}
$$</p><p>So for each point $x_i$ on the x-axis, we compute $\log p(x_i) - \log q(x_i)$ and multiply the result by $p(x_i)$. We then plot the resulting y-value in the right hand plot. This is how we get to the curve given in the right hand plot. The KL divergence is now defined as the <em>area under the graph</em>, which is shaded.</p><h2 id=kl-divergence-in-machine-learning-a-classanchor-idkl-in-mla>KL divergence in machine learning</h2><p>In most cases in machine learning we are given a dataset $X$ which was generated by some unknown probability distribution $P$. In this approach $P$ is considered to be the <em>target distribution</em> (that is, the &rsquo;true&rsquo; distribution) which we are trying to approximate using a distribution $Q$. We can evaluate candidate distributions $Q$ using the KL-divergence from $P$ to $Q$. In many cases, for example in variational inference, the KL divergence is used as an optimization criterion which is minimized in order to find the best candidate/approximation $Q$.</p><h2 id=interpreting-the-kullback-leibler-divergence-a-classanchor-idinterpreting-the-kullback-leibler-divergencea>Interpreting the Kullback Leibler divergence</h2><p>Note: Several different intepretations of the KL divergence exist. This interpretation describes a probabilistic perspective which is often useful for machine learning.</p><h3 id=expected-value>Expected value</h3><p>In order to understand how the KL divergence works, remember the formula for the <strong>expected value of a function</strong>. Given a function $f$ with $x$ being a discrete variable, the expected value of $f(x)$ is defined as</p><p>$$\mathbb{E}\big[f(x)\big] = \sum_x f(x) p(x)$$</p><p>where $p(x)$ is the probability density function of the variable $x$. For the continuous case we have</p><p>$$\mathbb{E}\big[f(x)\big] = \int_{-\infty}^{\infty}f(x) p(x) dx$$.</p><hr><p><strong>Example:</strong> Suppose you made a very good deal and bought three pairs of the best headphones for a reduced price of 200 USD each. You want to sell them for 350 USD each. We define the probability of selling $X=0, X=1, X=2, X=3$ headphones as follows:</p><p>$$p(X=0) = 0.1$$
$$p(X=1) = 0.2$$
$$p(X=3) = 0.3$$
$$p(X=4) = 0.4$$</p><p>We further define a function that measures profit: $f(x) = \text{revenue} - \text{cost} = 350*X - 200*X$. For example, when selling two pairs of headphones you will make: $f(X=2) = 700 - 400 = 300$. So what&rsquo;s our expected profit? We can compute it using the formula for the expected value:</p><p>$$
\begin{align}
\mathbb{E}\big[f(x)\big] &= p(X=0)*f(X=0) + p(X=1)*f(X=1) + p(X=2)*f(X=2) + p(X=3)*f(X=3) \\
&= 0.1 * 0 + 0.2 * 150 + 0.3 * 300 + 0.4 * 450 \\
&= $ 300<br>\end{align}
$$</p><hr><h3 id=ratio-of-px--qx>Ratio of p(x) / q(x)</h3><p>Looking back at the definition of the KL divergence we can see that it&rsquo;s quite similar to the definition of the expected value. When setting $f(x) = \log \big(\frac{p(x)}{q(x)}\big)$ we can see that:</p><p>$$
\begin{align}
\mathbb{E}\big[f(x)\big] &= \mathbb{E}_{x \sim p(x)}\big[\log \big(\frac{p(x)}{q(x)}\big) \big] \\
&= \int_{-\infty}^{\infty} p(x) \log \big( \frac{p(x)}{q(x)} \big) dx \\
&= D_{KL}(P || Q)
\end{align}
$$</p><p>But what does that mean? Let&rsquo;s start by looking at the quantity $\frac{p(x)}{q(x)}$. When having some probability density function $p$ and another probability density function $q$ we can compare the two by looking at the ratio of the two densities:</p><p>$$ratio = \frac{p(x)}{q(x)}$$</p><p>Insight: We can compare two probability density functions by means of the ratio.</p><p>Because both $p(x)$ and $q(x)$ are probability densities they output values between $0$ and $1$. When $q$ is similar to $p$, $q(x)$ should output values close to $p(x)$ for any input $x$.</p><hr><p><em><strong>Example:</strong></em> For some input $x_i$, $p(x_i)$ might be 0.78, i.e. $p(x_i) = 0.78$. Let&rsquo;s look at different densities $q$:</p><p>a) If $q$ and $p$ are identical, $q(x)$ would output the same value and the resulting ratio would be one:<br>$ratio = \frac{p(x)}{q(x)} = \frac{0.78}{0.78} = 1$</p><p>b) When $q(x)$ assigns a lower probability to the input $x$ than $p(x)$, the resulting ratio will be larger than one:<br>$ratio = \frac{p(x)}{q(x)} = \frac{0.78}{0.2} = 3.9$</p><p>c) When $q(x)$ assigns a higher probability to the input $x$ than $p(x)$, the resulting ratio will be smaller than one: $$ratio = \frac{p(x)}{q(x)} = \frac{0.78}{0.9} \approx 0.86$$</p><hr><p>The example provides an important insight:</p><blockquote><p><strong>Insight:</strong> For any input $x$ the value of the ratio tells us how much more likely $x$ is to occur under $p(x)$ compared to $q(x)$. A value of the ratio larger than 1 indicates that $p(x)$ is the more likely model. A value smaller than 1 indicates that $q$ is the more likely model.</p></blockquote><h3 id=ratio-for-entire-dataset>Ratio for entire dataset</h3><p>If we have a whole dataset $X = x_1, &mldr;, x_n$ we can compute the ratio of the entire set by taking the product over the individual ratios. Note: this only holds if the examples $x_i$ are independent of each other.</p><p>$$ratio = \prod_{i=1}^n \frac{p(x_i)}{q(x_i)}$$</p><p>To make the computation easier we can take the logarithm:</p><p>$$\text{log_ratio} = \sum_{i=1}^n \log \big( \frac{p(x_i)}{q(x_i)} \big)$$</p><p>When taking the logarithm, a log ratio value of 0 indicates that both models fit the data equally well. Values larger than 0 indicate that $p$ is the better model, that is, it fits the data better. Values smaller than 0 indicate that $q$ is the better model:</p><h3 id=example-calculation>Example calculation</h3><p>Let&rsquo;s calculate the KL divergence for our headphone example. We already have specified a distribution $P$ over the possible outcomes. Let&rsquo;s define another distribution $Q$ which expresses our belief that it&rsquo;s very likely that we sell all three pairs of headphones and less likely that we don&rsquo;t sell all of them (or none).</p><table><thead><tr><th style=text-align:center>Distribution</th><th style=text-align:center>$X = 0 \quad$</th><th style=text-align:center>$X=1 \quad$</th><th style=text-align:center>$X=2 \quad$</th><th style=text-align:center>$X=3 \quad$</th></tr></thead><tbody><tr><td style=text-align:center>$P(X)$</td><td style=text-align:center>0.1</td><td style=text-align:center>0.2</td><td style=text-align:center>0.3</td><td style=text-align:center>0.4</td></tr><tr><td style=text-align:center>$Q(X)$</td><td style=text-align:center>0.1</td><td style=text-align:center>0.1</td><td style=text-align:center>0.1</td><td style=text-align:center>0.7</td></tr></tbody></table><p>How similar are $P$ and $Q$? Let&rsquo;s compute the KL divergence:
$$
\begin{align}
D_{KL}(P || Q) &= \sum_{x \in X} P(x) \log \big( \frac{P(x)}{Q(x)} \big) \\
&= 0.1 * \log \big( \frac{0.1}{0.1} \big) + 0.2 * \log \big( \frac{0.2}{0.1} \big) + 0.3 * \log \big( \frac{0.3}{0.1} \big) + 0.4 * \log \big( \frac{0.4}{0.7} \big) \\
&\approx 0.244
\end{align}
$$</p><h3 id=ratio-vs-kl-divergence>Ratio vs. KL-divergence</h3><p>We discovered that the log-ratio can be used to compare two probabilty densities $p$ and $q$. The KL divergence is nothing else than the expected value of the log-ratio. When setting $f(x) = \log \big(\frac{p(x)}{q(x)}\big)$ we receive:</p><p>$$
\begin{align}
\mathbb{E}\big[f(x)\big] &= \mathbb{E}_{x \sim p(x)}\big[\log \big(\frac{p(x)}{q(x)}\big) \big] \\
&= \int_{-\infty}^{\infty} p(x) \log \big( \frac{p(x)}{q(x)} \big) dx \\
&= D_{KL}(P || Q)
\end{align}
$$</p><blockquote><p><strong>Insight:</strong> The KL divergence is simply the expected value of the log-ratio of the entire dataset.</p></blockquote><h2 id=why-is-the-kl-divergence-always-non-negative>Why is the KL divergence always non-negative?</h2><p>An important property of the KL divergence is that it&rsquo;s always non-negative, i.e. $D_{KL}(P , || , Q) \ge 0$ for any valid $P, Q$. We can prove this using <a href=https://en.wikipedia.org/wiki/Jensen%27s_inequality target=_blank rel=noopener>Jensen&rsquo;s inequality</a>.</p><p>Jensen&rsquo;s inequality states that, if a function $f(x)$ is convex, then</p><p>$$\mathbb{E}[f(x)] \ge f(\mathbb{E}[x])$$</p><p>To show that $D_{KL}(P , || , Q) \ge 0$ we first make use of the expected value:</p><p>$$
\begin{align}
D_{KL}(P || Q) &= \int_{-\infty}^{\infty} p(x) \log \big( \frac{p(x)}{q(x)} \big) dx \\
&= \mathbb{E}_{x \sim p(x)}\big[\log \big(\frac{p(x)}{q(x)}\big) \big] \\
&= - \mathbb{E}_{x \sim p(x)}\big[\log \big(\frac{q(x)}{p(x)}\big) \big]
\end{align}
$$</p><p>Because $-\log(x)$ is a convex function we can apply Jensen&rsquo;s inequality:</p><p>$$
\begin{align}
-\mathbb{E}_{x \sim p(x)}\big[\log \big(\frac{q(x)}{p(x)}\big) \big] &\ge - \log \big( \mathbb{E}_{x \sim p(x)}\big[\frac{q(x)}{p(x)} \big] \big) \\
&= - \log \big( \int_{-\infty}^{\infty} p(x) \frac{q(x)}{p(x)} dx \big) \\
&= - \log \big( \int_{-\infty}^{\infty} q(x) dx \big) \\
&= - \log(1) \\
&= 0
\end{align}
$$</p><h2 id=which-type-of-logarithm-to-use>Which type of logarithm to use</h2><p>It&rsquo;s interesting to note that we can use different bases for the logarithm in the definition of the KL divergence, depending on the interpretation. For example, when using the natural logarithm the result of the KL divergence is measured in so called &rsquo;nats&rsquo;. When using the logarithm to base 2 the result is measured in bits.</p></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/zotroneneis/zotroneneis.github.io/edit/main/content/posts/machine_learning/kl_divergence.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
Improve this page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/machine_learning/bayesian_linear_regression_2/ title="Bayesian linear regression 2" class="btn filled-button"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>Bayesian linear regression 2</div></a></div><div class="col-md-6 next-article"><a href=/posts/machine_learning/principal_component_analysis/ title="Principal component analysis (PCA)" class="btn filled-button"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>Principal component analysis (PCA)</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn data-toggle=tooltip data-placement=left title="Scroll to top"><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#definition>Definition</a></li><li><a href=#visual-example-a-classanchor-idvisual-examplea>Visual example</a></li><li><a href=#kl-divergence-in-machine-learning-a-classanchor-idkl-in-mla>KL divergence in machine learning</a></li><li><a href=#interpreting-the-kullback-leibler-divergence-a-classanchor-idinterpreting-the-kullback-leibler-divergencea>Interpreting the Kullback Leibler divergence</a><ul><li><a href=#expected-value>Expected value</a></li><li><a href=#ratio-of-px--qx>Ratio of p(x) / q(x)</a></li><li><a href=#ratio-for-entire-dataset>Ratio for entire dataset</a></li><li><a href=#example-calculation>Example calculation</a></li><li><a href=#ratio-vs-kl-divergence>Ratio vs. KL-divergence</a></li></ul></li><li><a href=#why-is-the-kl-divergence-always-non-negative>Why is the KL divergence always non-negative?</a></li><li><a href=#which-type-of-logarithm-to-use>Which type of logarithm to use</a></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://alpopkes.com/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://alpopkes.com/#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=https://alpopkes.com/#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=https://alpopkes.com/#education>Education</a></li><li class=nav-item><a class=smooth-scroll href=https://alpopkes.com/#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=https://alpopkes.com/#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=https://alpopkes.com/#appearances>Talks & Podcasts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:popkes@gmx.net target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>popkes@gmx.net</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hugo-toha/toha target=_blank rel=noopener><img src=/images/theme-logo_hu16779671404603505019.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020-2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/application.a01d837827c5183eb1439ce1afe5db5896d8b892671bcbe43996542de67b6510.js integrity="sha256-oB2DeCfFGD6xQ5zhr+XbWJbYuJJnG8vkOZZULeZ7ZRA=" defer></script></body></html>