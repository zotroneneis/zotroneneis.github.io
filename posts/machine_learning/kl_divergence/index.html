<!doctype html><html><head><title>Kullback-Leibler Divergence</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/navigators/navbar.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/images/author/image_al_hu6aa485cfd64a98f54b411e3a0324b396_178285_42x0_resize_q75_box.jpg><link rel=stylesheet href=/css/style.css><meta name=description content="Kullback-Leibler Divergence"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/images/author/image_al_hu6aa485cfd64a98f54b411e3a0324b396_178285_42x0_resize_q75_box.jpg>Anna-Lena Popkes</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/images/author/image_al_hu6aa485cfd64a98f54b411e3a0324b396_178285_42x0_resize_q75_box.jpg class=d-none id=main-logo>
<img src=/images/author/image_al_hu6aa485cfd64a98f54b411e3a0324b396_178285_42x0_resize_q75_box.jpg class=d-none id=inverted-logo></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><input type=text placeholder=Search data-search id=search-box><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><a href=/posts/my_path_to_ml/>My path to machine learning</a></li><li><i class="fas fa-plus-circle"></i><a href=/posts/books/>Books</a><ul><li><a href=/posts/books/reading_list/>Personal reading List</a></li><li><a href=/posts/books/deep_work/>Deep work</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/machine_learning/>Machine Learning</a><ul class=active><li><a class=active href=/posts/machine_learning/kl_divergence/>KL Divergence</a></li><li><a href=/posts/machine_learning/variational_inference/>Variational Inference</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/python/>Python</a><ul><li><a href=/posts/python/mocking/>Mocking</a></li><li><i class="fas fa-plus-circle"></i><a href=/posts/python/magical_universe/>Magical Universe</a><ul><li><a href=/posts/python/magical_universe/day_1_start/>Start</a></li><li><a href=/posts/python/magical_universe/day_1_magical_universe/>The Tales of Castle Kilmere</a></li><li><a href=/posts/python/magical_universe/day_1_first_post_oop/>Object-oriented programming</a></li><li><a href=/posts/python/magical_universe/day_2_types_of_methods/>Types of methods</a></li><li><a href=/posts/python/magical_universe/day_3_type_annotations/>Type annotations</a></li><li><a href=/posts/python/magical_universe/day_4_to_string_conversion/>To-string conversion</a></li><li><a href=/posts/python/magical_universe/day_5_decorators/>Decorators</a></li><li><a href=/posts/python/magical_universe/day_6_properties/>Properties</a></li><li><a href=/posts/python/magical_universe/day_7_underscore_patterns/>Underscore patterns</a></li><li><a href=/posts/python/magical_universe/day_8_extending_universe/>Extending the universe</a></li><li><a href=/posts/python/magical_universe/day_9_duck_typing/>Duck Typing</a></li><li><a href=/posts/python/magical_universe/day_10_11_namedtuples/>Namedtuples</a></li><li><a href=/posts/python/magical_universe/day_12_to_15_abcs/>Abstract Base Classes</a></li><li><a href=/posts/python/magical_universe/day_16_to_18_data_classes/>Data classes</a></li><li><a href=/posts/python/magical_universe/day_19_immutable_data_classes/>Immutable data classes</a></li><li><a href=/posts/python/magical_universe/day_20_decorators_in_classes/>Decorators in classes</a></li><li><a href=/posts/python/magical_universe/day_21_if_main/>if __name__ == "__main__"</a></li><li><a href=/posts/python/magical_universe/day_22_to_24_context_managers/>Context managers</a></li><li><a href=/posts/python/magical_universe/day_25_to_28_pytest/>Testing with pytest</a></li><li><a href=/posts/python/magical_universe/day_29_to_31_iterators/>Iterators</a></li><li><a href=/posts/python/magical_universe/day_34_multisets/>Multisets</a></li><li><a href=/posts/python/magical_universe/day_37_extending_universe/>Extending the universe II</a></li><li><a href=/posts/python/magical_universe/day_43_to_45_exception_classes/>Exception classes</a></li><li><a href=/posts/python/magical_universe/day_46_functools_wraps/>functools.wraps</a></li><li><a href=/posts/python/magical_universe/day_47_to_48_defaultdict/>Defaultdict</a></li><li><a href=/posts/python/magical_universe/day_49_to_50_config_files/>Config files</a></li><li><a href=/posts/python/magical_universe/2018-09-16-blog-post-day-51/>Wrap up</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/software_engineering/>Software Engineering</a><ul><li><a href=/posts/software_engineering/containers/>Intro to containers</a></li><li><a href=/posts/software_engineering/docker/>Intro to Docker</a></li><li><a href=/posts/software_engineering/virtual_machines/>Intro to virtual machines</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://alpopkes.com/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/image_al.jpg><h5 class=author-name>Anna-Lena Popkes</h5><p>February 2, 2019</p></div><div class=title><h1>Kullback-Leibler Divergence</h1></div><div class=post-content id=post-content><p>One of the points on my long &lsquo;stuff-you-have-to-look-at&rsquo; list is the Kullback-Leibler divergence. I finally took the time to take a detailed look at this topic.</p><h2 id=definition>Definition</h2><p>The KL-divergence is a measure of how similar (or different) two probablity distributions are. When having a discrete probability distribution $P$ and another probability distribution $Q$ the KL-divergence for a set of points $X$ is defined as:</p><p>$$D_{KL}(P ,|| ,Q) = \sum_{x \in X} P(x) \log \big( \frac{P(x)}{Q(x)} \big)$$</p><p>For probability distributions over continuous variables the sum turns into an integral:</p><p>$$ D_{KL}(P ,|| ,Q) = \int_{-\infty}^{\infty} p(x) \log \big( \frac{p(x)}{q(x)} \big) dx$$</p><p>where $p$ and $q$ denote the probability density functions of $P$ and $Q$.</p><p><a href=https://en.wikipedia.org/wiki/Kullback_Leibler_divergence>Source</a></p><h2 id=visual-example-a-classanchor-idvisual-examplea>Visual example</h2><p>The <a href=https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence>Wikipedia entry</a> on the KL divergence contains a nice illustration:</p><img src=/posts/machine_learning/images/kl_divergence.png width=80% class=center><p>On the left hand side we can see two Gaussian probability density functions $p(x)$ and $q(x)$. The right hand side show the area that is integrated when computing the KL divergence from $p$ to $q$. We know that:</p><p>$$
\begin{align}
D_{KL}(P ,|| ,Q) &= \int_{-\infty}^{\infty} p(x) \log \big( \frac{p(x)}{q(x)} \big) dx \<br>&= \int_{-\infty}^{\infty} p(x) \big( \log p(x) - \log q(x) \big) dx
\end{align}
$$</p><p>So for each point $x_i$ on the x-axis, we compute $\log p(x_i) - \log q(x_i)$ and multiply the result by $p(x_i)$. We then plot the resulting y-value in the right hand plot. This is how we get to the curve given in the right hand plot. The KL divergence is now defined as the <em>area under the graph</em>, which is shaded.</p><h2 id=kl-divergence-in-machine-learning-a-classanchor-idkl-in-mla>KL divergence in machine learning</h2><p>In most cases in machine learning we are given a dataset $X$ which was generated by some unknown probability distribution $P$. In this approach $P$ is considered to be the <em>target distribution</em> (that is, the &lsquo;true&rsquo; distribution) which we are trying to approximate using a distribution $Q$. We can evaluate candidate distributions $Q$ using the KL-divergence from $P$ to $Q$. In many cases, for example in variational inference, the KL divergence is used as an optimization criterion which is minimized in order to find the best candidate/approximation $Q$.</p><h2 id=interpreting-the-kullback-leibler-divergence-a-classanchor-idinterpreting-the-kullback-leibler-divergencea>Interpreting the Kullback Leibler divergence</h2><p>Note: Several different intepretations of the KL divergence exist. This interpretation describes a probabilistic perspective which is often useful for machine learning.</p><h3 id=expected-value>Expected value</h3><p>In order to understand how the KL divergence works, remember the formula for the <strong>expected value of a function</strong>. Given a function $f$ with $x$ being a discrete variable, the expected value of $f(x)$ is defined as</p><p>$$\mathbb{E}\big[f(x)\big] = \sum_x f(x) p(x)$$</p><p>where $p(x)$ is the probability density function of the variable $x$. For the continuous case we have</p><p>$$\mathbb{E}\big[f(x)\big] = \int_{-\infty}^{\infty}f(x) p(x) dx$$.</p><hr><p><strong>Example:</strong> Suppose you made a very good deal and bought three pairs of the best headphones for a reduced price of 200 USD each. You want to sell them for 350 USD each. We define the probability of selling $X=0, X=1, X=2, X=3$ headphones as follows:</p><p>$$p(X=0) = 0.1$$
$$p(X=1) = 0.2$$
$$p(X=3) = 0.3$$
$$p(X=4) = 0.4$$</p><p>We further define a function that measures profit: $f(x) = \text{revenue} - \text{cost} = 350*X - 200*X$. For example, when selling two pairs of headphones you will make: $f(X=2) = 700 - 400 = 300$. So what&rsquo;s our expected profit? We can compute it using the formula for the expected value:</p><p>$$
\begin{align}
\mathbb{E}\big[f(x)\big] &= p(X=0)*f(X=0) + p(X=1)*f(X=1) + p(X=2)*f(X=2) + p(X=3)*f(X=3) \\<br>&= 0.1 * 0 + 0.2 * 150 + 0.3 * 300 + 0.4 * 450 \\<br>&= $ 300<br>\end{align}
$$</p><hr><h3 id=ratio-of-px--qx>Ratio of p(x) / q(x)</h3><p>Looking back at the definition of the KL divergence we can see that it&rsquo;s quite similar to the definition of the expected value. When setting $f(x) = \log \big(\frac{p(x)}{q(x)}\big)$ we can see that:</p><p>$$
\begin{align}
\mathbb{E}\big[f(x)\big] &= \mathbb{E}_{x \sim p(x)}\big[\log \big(\frac{p(x)}{q(x)}\big) \big] \\<br>&= \int_{-\infty}^{\infty} p(x) \log \big( \frac{p(x)}{q(x)} \big) dx \\<br>&= D_{KL}(P || Q)
\end{align}
$$</p><p>But what does that mean? Let&rsquo;s start by looking at the quantity $\frac{p(x)}{q(x)}$. When having some probability density function $p$ and another probability density function $q$ we can compare the two by looking at the ratio of the two densities:</p><p>$$ratio = \frac{p(x)}{q(x)}$$</p><p>Insight: We can compare two probability density functions by means of the ratio.</p><p>Because both $p(x)$ and $q(x)$ are probability densities they output values between $0$ and $1$. When $q$ is similar to $p$, $q(x)$ should output values close to $p(x)$ for any input $x$.</p><hr><p><em><strong>Example:</strong></em> For some input $x_i$, $p(x_i)$ might be 0.78, i.e. $p(x_i) = 0.78$. Let&rsquo;s look at different densities $q$:</p><p>a) If $q$ and $p$ are identical, $q(x)$ would output the same value and the resulting ratio would be one:<br>$ratio = \frac{p(x)}{q(x)} = \frac{0.78}{0.78} = 1$</p><p>b) When $q(x)$ assigns a lower probability to the input $x$ than $p(x)$, the resulting ratio will be larger than one:<br>$ratio = \frac{p(x)}{q(x)} = \frac{0.78}{0.2} = 3.9$</p><p>c) When $q(x)$ assigns a higher probability to the input $x$ than $p(x)$, the resulting ratio will be smaller than one: $$ratio = \frac{p(x)}{q(x)} = \frac{0.78}{0.9} \approx 0.86$$</p><hr><p>The example provides an important insight:</p><blockquote><p><strong>Insight:</strong> For any input $x$ the value of the ratio tells us how much more likely $x$ is to occur under $p(x)$ compared to $q(x)$. A value of the ratio larger than 1 indicates that $p(x)$ is the more likely model. A value smaller than 1 indicates that $q$ is the more likely model.</p></blockquote><h3 id=ratio-for-entire-dataset>Ratio for entire dataset</h3><p>If we have a whole dataset $X = x_1, &mldr;, x_n$ we can compute the ratio of the entire set by taking the product over the individual ratios. Note: this only holds if the examples $x_i$ are independent of each other.</p><p>$$ratio = \prod_{i=1}^n \frac{p(x_i)}{q(x_i)}$$</p><p>To make the computation easier we can take the logarithm:</p><p>$$\text{log_ratio} = \sum_{i=1}^n \log \big( \frac{p(x_i)}{q(x_i)} \big)$$</p><p>When taking the logarithm, a log ratio value of 0 indicates that both models fit the data equally well. Values larger than 0 indicate that $p$ is the better model, that is, it fits the data better. Values smaller than 0 indicate that $q$ is the better model:</p><p></p><h3 id=example-calculation>Example calculation</h3><p>Let&rsquo;s calculate the KL divergence for our headphone example. We already have specified a distribution $P$ over the possible outcomes. Let&rsquo;s define another distribution $Q$ which expresses our belief that it&rsquo;s very likely that we sell all three pairs of headphones and less likely that we don&rsquo;t sell all of them (or none).</p><table><thead><tr><th style=text-align:center>Distribution</th><th style=text-align:center>$X = 0 \quad$</th><th style=text-align:center>$X=1 \quad$</th><th style=text-align:center>$X=2 \quad$</th><th style=text-align:center>$X=3 \quad$</th></tr></thead><tbody><tr><td style=text-align:center>$P(X)$</td><td style=text-align:center>0.1</td><td style=text-align:center>0.2</td><td style=text-align:center>0.3</td><td style=text-align:center>0.4</td></tr><tr><td style=text-align:center>$Q(X)$</td><td style=text-align:center>0.1</td><td style=text-align:center>0.1</td><td style=text-align:center>0.1</td><td style=text-align:center>0.7</td></tr></tbody></table><p>How similar are $P$ and $Q$? Let&rsquo;s compute the KL divergence:
$$
\begin{align}
D_{KL}(P || Q) &= \sum_{x \in X} P(x) \log \big( \frac{P(x)}{Q(x)} \big) \\<br>&= 0.1 * \log \big( \frac{0.1}{0.1} \big) + 0.2 * \log \big( \frac{0.2}{0.1} \big) + 0.3 * \log \big( \frac{0.3}{0.1} \big) + 0.4 * \log \big( \frac{0.4}{0.7} \big) \\<br>&\approx 0.244
\end{align}
$$</p><h3 id=ratio-vs-kl-divergence>Ratio vs. KL-divergence</h3><p>We discovered that the log-ratio can be used to compare two probabilty densities $p$ and $q$. The KL divergence is nothing else than the expected value of the log-ratio. When setting $f(x) = \log \big(\frac{p(x)}{q(x)}\big)$ we receive:</p><p>$$
\begin{align}
\mathbb{E}\big[f(x)\big] &= \mathbb{E}_{x \sim p(x)}\big[\log \big(\frac{p(x)}{q(x)}\big) \big] \\<br>&= \int_{-\infty}^{\infty} p(x) \log \big( \frac{p(x)}{q(x)} \big) dx \\<br>&= D_{KL}(P || Q)
\end{align}
$$</p><blockquote><p><strong>Insight:</strong> The KL divergence is simply the expected value of the log-ratio of the entire dataset.</p></blockquote><h2 id=why-is-the-kl-divergence-always-non-negative>Why is the KL divergence always non-negative?</h2><p>An important property of the KL divergence is that it&rsquo;s always non-negative, i.e. $D_{KL}(P , || , Q) \ge 0$ for any valid $P, Q$. We can prove this using <a href=https://en.wikipedia.org/wiki/Jensen%27s_inequality>Jensen&rsquo;s inequality</a>.</p><p>Jensen&rsquo;s inequality states that, if a function $f(x)$ is convex, then</p><p>$$\mathbb{E}[f(x)] \ge f(\mathbb{E}[x])$$</p><p>To show that $D_{KL}(P , || , Q) \ge 0$ we first make use of the expected value:</p><p>$$
\begin{align}
D_{KL}(P || Q) &= \int_{-\infty}^{\infty} p(x) \log \big( \frac{p(x)}{q(x)} \big) dx \\<br>&= \mathbb{E}_{x \sim p(x)}\big[\log \big(\frac{p(x)}{q(x)}\big) \big] \\<br>&= - \mathbb{E}_{x \sim p(x)}\big[\log \big(\frac{q(x)}{p(x)}\big) \big]
\end{align}
$$</p><p>Because $-\log(x)$ is a convex function we can apply Jensen&rsquo;s inequality:</p><p>$$
\begin{align}
-\mathbb{E}_{x \sim p(x)}\big[\log \big(\frac{q(x)}{p(x)}\big) \big] &\ge - \log \big( \mathbb{E}_{x \sim p(x)}\big[\frac{q(x)}{p(x)} \big] \big) \\<br>&= - \log \big( \int_{-\infty}^{\infty} p(x) \frac{q(x)}{p(x)} dx \big) \\
&= - \log \big( \int_{-\infty}^{\infty} q(x) dx \big) \\<br>&= - \log(1) \\<br>&= 0
\end{align}
$$</p><h2 id=which-type-of-logarithm-to-use>Which type of logarithm to use</h2><p>It&rsquo;s interesting to note that we can use different bases for the logarithm in the definition of the KL divergence, depending on the interpretation. For example, when using the natural logarithm the result of the KL divergence is measured in so called &lsquo;nats&rsquo;. When using the logarithm to base 2 the result is measured in bits.</p></div><div class=btn-improve-page><a href=https://github.com/zotroneneis/zotroneneis.github.io/edit/main/content/posts/machine_learning/kl_divergence.md><i class="fas fa-code-branch"></i>Improve this page</a></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/machine_learning/variational_inference/ class="btn btn-outline-info"><span><i class="fas fa-chevron-circle-left"></i>Prev</span><br><span>Variational Inference</span></a></div><div class="col-md-6 next-article"><a href=/posts/software_engineering/docker/ class="btn btn-outline-info"><span>Next <i class="fas fa-chevron-circle-right"></i></span><br><span>An introduction to Docker</span></a></div></div><hr></div></div></div></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#definition>Definition</a></li><li><a href=#visual-example-a-classanchor-idvisual-examplea>Visual example</a></li><li><a href=#kl-divergence-in-machine-learning-a-classanchor-idkl-in-mla>KL divergence in machine learning</a></li><li><a href=#interpreting-the-kullback-leibler-divergence-a-classanchor-idinterpreting-the-kullback-leibler-divergencea>Interpreting the Kullback Leibler divergence</a><ul><li><a href=#expected-value>Expected value</a></li><li><a href=#ratio-of-px--qx>Ratio of p(x) / q(x)</a></li><li><a href=#ratio-for-entire-dataset>Ratio for entire dataset</a></li><li><a href=#example-calculation>Example calculation</a></li><li><a href=#ratio-vs-kl-divergence>Ratio vs. KL-divergence</a></li></ul></li><li><a href=#why-is-the-kl-divergence-always-non-negative>Why is the KL divergence always non-negative?</a></li><li><a href=#which-type-of-logarithm-to-use>Which type of logarithm to use</a></li></ul></nav></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=#about>About</a></li><li class=nav-item><a class=smooth-scroll href=#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=#education>Education</a></li><li class=nav-item><a class=smooth-scroll href=#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span>Email:</span> <span>popkes@gmx.net</span></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=#><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png>
Toha</a></div><div class="col-md-4 text-center">Â© 2020-2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/js/jquery-3.4.1.min.js></script><script src=/js/popper.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/navbar.js></script><script src=/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad();</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],processEnvironments:true}};</script></body></html>