<!doctype html><html lang=en><head><title>Variational Inference</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/application.b0c81e8d5a19b7c855a22c50bdb6f3ed82d31ad8b723e6b49096e34f22acdbbb.css integrity="sha256-sMgejVoZt8hVoixQvbbz7YLTGti3I+a0kJbjTyKs27s="><link rel=icon type=image/png href=/images/author/image_al_hu15845133475761513827.jpg><meta property="og:url" content="https://alpopkes.com/posts/machine_learning/variational_inference/"><meta property="og:site_name" content="Anna-Lena Popkes"><meta property="og:title" content="Variational Inference"><meta property="og:description" content="Introduction Variational inference is an important topic that is widely used in machine learning. For example, it’s the basis for variational autoencoders. Also Bayesian learning often makes use variational of inference. To understand what variational inference is, how it works and why it’s useful we will go through each point step by step.
What are latent variables? A latent variable is the opposite of an observed variable. This means that a latent variable is not directly observed but inferred from other variables which are observed."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-02-23T00:00:00+00:00"><meta property="article:modified_time" content="2019-02-23T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Variational Inference"><meta name=twitter:description content="Introduction Variational inference is an important topic that is widely used in machine learning. For example, it’s the basis for variational autoencoders. Also Bayesian learning often makes use variational of inference. To understand what variational inference is, how it works and why it’s useful we will go through each point step by step.
What are latent variables? A latent variable is the opposite of an observed variable. This means that a latent variable is not directly observed but inferred from other variables which are observed."><meta name=description content="Variational Inference"><script>theme=localStorage.getItem("darkmode:color-scheme")||"system",theme=="system"&&(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?theme="dark":theme="light"),document.documentElement.setAttribute("data-theme",theme)</script></head><body class="type-posts kind-page" data-bs-spy=scroll data-bs-target=#TableOfContents data-bs-offset=80><div class="container-fluid bg-secondary wrapper"><nav class="navbar navbar-expand-xl top-navbar shadow" id=top-navbar><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button>
<i data-feather=sidebar></i>
</button>
<a class=navbar-brand href=/><img src=/images/author/image_al_hu15845133475761513827.jpg id=logo alt=Logo>
Anna-Lena Popkes</a>
<button class="navbar-toggler navbar-light" id=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#top-nav-items aria-label=menu>
<i data-feather=menu></i></button><div class="collapse navbar-collapse dynamic-navbar" id=top-nav-items><ul class="nav navbar-nav ms-auto"><li class=nav-item><a class=nav-link href=/#home>Home</a></li><li class=nav-item><a class=nav-link href=/#about>About</a></li><li class=nav-item><a class=nav-link href=/#skills>Skills</a></li><li class=nav-item><a class=nav-link href=/#experiences>Experiences</a></li><li class=nav-item><a class=nav-link href=/#projects>Projects</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>More</a><div class=dropdown-menu aria-labelledby=navbarDropdown><a class=dropdown-item href=/#projects>Projects</a>
<a class=dropdown-item href=/#appearances>Talks & Podcasts</a>
<a class=dropdown-item href=/#recent-posts>Recent Posts</a></div></li><div id=top-navbar-divider></div><li class=nav-item><a class=nav-link id=blog-link href=/posts>Posts</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg class=theme-icon src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme"></a><div id=themeMenu class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# data-scheme=light><img class=theme-icon src=/icons/sun-svgrepo-com.svg width=20 alt="Light Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=dark><img class=theme-icon src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=system><img class=theme-icon src=/icons/computer-svgrepo-com.svg width=20 alt="System Theme"></a></div></li></ul></div></div><img src=/images/author/image_al_hu15845133475761513827.jpg class=d-none id=main-logo alt=Logo>
<img src=/images/author/image_al_hu15845133475761513827.jpg class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts/ data-filter=all>Posts</a></li><div class=subtree><li><a class=list-link href=/posts/news/ title="Personal news">Personal news</a></li><li><a class=list-link href=/posts/my_path_to_ml/ title="My path to machine learning">My path to machine learning</a></li><li><i data-feather=plus-circle></i><a class=list-link href=/posts/books/> Books</a><ul><li><a class=list-link href=/posts/books/reading_list/ title="Personal reading List">Personal reading List</a></li><li><a class=list-link href=/posts/books/deep_work/ title="Deep work">Deep work</a></li></ul></li><li><i data-feather=minus-circle></i><a class="active list-link" href=/posts/machine_learning/> Machine Learning</a><ul class=active><li><a class=list-link href=/posts/machine_learning/bayesian_linear_regression/ title="Bayesian linear regression">Bayesian linear regression</a></li><li><a class=list-link href=/posts/machine_learning/kl_divergence/ title="KL Divergence">KL Divergence</a></li><li><a class=list-link href=/posts/machine_learning/principal_component_analysis/ title="Principal component analysis (PCA)">Principal component analysis (PCA)</a></li><li><a class=list-link href=/posts/machine_learning/support_vector_machines/ title="Support vector machines">Support vector machines</a></li><li><a class="active list-link" href=/posts/machine_learning/variational_inference/ title="Variational Inference">Variational Inference</a></li></ul></li><li><i data-feather=plus-circle></i><a class=list-link href=/posts/python/> Python</a><ul><li><a class=list-link href=/posts/python/coding_with_kids/ title="Coding with kids">Coding with kids</a></li><li><a class=list-link href=/posts/python/mocking/ title=Mocking>Mocking</a></li><li><a class=list-link href=/posts/python/packaging_tools/ title="Packaging tools">Packaging tools</a></li><li><i data-feather=plus-circle></i><a class=list-link href=/posts/python/magical_universe/> Magical Universe</a><ul><li><a class=list-link href=/posts/python/magical_universe/day_1_start/ title=Start>Start</a></li><li><a class=list-link href=/posts/python/magical_universe/day_1_magical_universe/ title="The Tales of Castle Kilmere">The Tales of Castle Kilmere</a></li><li><a class=list-link href=/posts/python/magical_universe/day_1_first_post_oop/ title="Object-oriented programming">Object-oriented programming</a></li><li><a class=list-link href=/posts/python/magical_universe/day_2_types_of_methods/ title="Types of methods">Types of methods</a></li><li><a class=list-link href=/posts/python/magical_universe/day_3_type_annotations/ title="Type annotations">Type annotations</a></li><li><a class=list-link href=/posts/python/magical_universe/day_4_to_string_conversion/ title="To-string conversion">To-string conversion</a></li><li><a class=list-link href=/posts/python/magical_universe/day_5_decorators/ title=Decorators>Decorators</a></li><li><a class=list-link href=/posts/python/magical_universe/day_6_properties/ title=Properties>Properties</a></li><li><a class=list-link href=/posts/python/magical_universe/day_7_underscore_patterns/ title="Underscore patterns">Underscore patterns</a></li><li><a class=list-link href=/posts/python/magical_universe/day_8_extending_universe/ title="Extending the universe">Extending the universe</a></li><li><a class=list-link href=/posts/python/magical_universe/day_9_duck_typing/ title="Duck Typing">Duck Typing</a></li><li><a class=list-link href=/posts/python/magical_universe/day_10_11_namedtuples/ title=Namedtuples>Namedtuples</a></li><li><a class=list-link href=/posts/python/magical_universe/day_12_to_15_abcs/ title="Abstract Base Classes">Abstract Base Classes</a></li><li><a class=list-link href=/posts/python/magical_universe/day_16_to_18_data_classes/ title="Data classes">Data classes</a></li><li><a class=list-link href=/posts/python/magical_universe/day_19_immutable_data_classes/ title="Immutable data classes">Immutable data classes</a></li><li><a class=list-link href=/posts/python/magical_universe/day_20_decorators_in_classes/ title="Decorators in classes">Decorators in classes</a></li><li><a class=list-link href=/posts/python/magical_universe/day_21_if_main/ title='if __name__ == "__main__"'>if __name__ == "__main__"</a></li><li><a class=list-link href=/posts/python/magical_universe/day_22_to_24_context_managers/ title="Context managers">Context managers</a></li><li><a class=list-link href=/posts/python/magical_universe/day_25_to_28_pytest/ title="Testing with pytest">Testing with pytest</a></li><li><a class=list-link href=/posts/python/magical_universe/day_29_to_31_iterators/ title=Iterators>Iterators</a></li><li><a class=list-link href=/posts/python/magical_universe/day_34_multisets/ title=Multisets>Multisets</a></li><li><a class=list-link href=/posts/python/magical_universe/day_37_extending_universe/ title="Extending the universe II">Extending the universe II</a></li><li><a class=list-link href=/posts/python/magical_universe/day_43_to_45_exception_classes/ title="Exception classes">Exception classes</a></li><li><a class=list-link href=/posts/python/magical_universe/day_46_functools_wraps/ title=functools.wraps>functools.wraps</a></li><li><a class=list-link href=/posts/python/magical_universe/day_47_to_48_defaultdict/ title=Defaultdict>Defaultdict</a></li><li><a class=list-link href=/posts/python/magical_universe/day_49_to_50_config_files/ title="Config files">Config files</a></li><li><a class=list-link href=/posts/python/magical_universe/2018-09-16-blog-post-day-51/ title="Wrap up">Wrap up</a></li></ul></li></ul></li><li><i data-feather=plus-circle></i><a class=list-link href=/posts/software_engineering/> Software Engineering</a><ul><li><a class=list-link href=/posts/software_engineering/containers/ title="Intro to containers">Intro to containers</a></li><li><a class=list-link href=/posts/software_engineering/docker/ title="Intro to Docker">Intro to Docker</a></li><li><a class=list-link href=/posts/software_engineering/virtual_machines/ title="Intro to virtual machines">Intro to virtual machines</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ms-auto align-self-lg-center"><img class=rounded-circle src=/images/author/image_al_hu6187147171234605626.jpg alt="Author Image"><h5 class=author-name>Anna-Lena Popkes</h5><p class=text-muted>Saturday, February 23, 2019</p></div><div class=title><h1>Variational Inference</h1></div><div class=post-content id=post-content><h1 id=introduction>Introduction</h1><p>Variational inference is an important topic that is widely used in machine learning. For example, it&rsquo;s the basis for variational autoencoders. Also Bayesian learning often makes use variational of inference. To understand what variational inference is, how it works and why it&rsquo;s useful we will go through each point step by step.</p><h2 id=what-are-latent-variables>What are latent variables?</h2><p>A latent variable is the opposite of an <em>observed</em> variable. This means that a latent variable is not directly observed but inferred from other variables which are observed. <a href=https://learnche.org/pid/contents target=_blank rel=noopener>This book</a> provides a nice conceptual example:</p><blockquote><p><strong>Example:</strong>
Consider your overall health. There are multiple measurements we can use to assess health, were each measurement is looking at a certain physical property, for example blood pressure or body temperature. However, &lsquo;health&rsquo; remains an abstract concept that cannot be measured directly. In this sense, health is a latent variable, whereas blood pressure and body temperature are observable variables.</p></blockquote><h2 id=what-is-variational-inference>What is variational inference?</h2><p>Variational inference is a machine learning method which allows us to approximate probability distributions. In many real world problems we are faced with probability distributions that can&rsquo;t be computed. This especially often happens when a distribution involves latent variables. Therefore, we need strategies to approximate such distributions. Variational inference is one method for doing this. Several other methods exist which broadly fall into two classes: methods that rely on <em>stochastic</em> approximations (like <a href=https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo target=_blank rel=noopener>Markov chain Monte Carlo sampling</a>) and those that rely on deterministic approximations (like variational inference).</p><p>A key characteristic of variational inference is that it <em>reframes</em> the original problem (computing some probability distribution) into a simpler problem which can be solved. Different to this, Markov chain Monte Carlo sampling directly approximates the target distribution by sampling from it.</p><p>A high level example for variational inference is given <a href=https://www.quora.com/What-is-variational-inference target=_blank rel=noopener>here</a>:</p><blockquote><p><strong>Example:</strong> Variational inference is similar to what often happens when attending a presentation or lecture. Someone in the audience asks the presenter a very difficult answer which she can&rsquo;t answer. Instead of answering the original, difficult question, the presenter reframes the question into an easier one which can be answered exactly.</p></blockquote><h2 id=problem-set-up>Problem set-up</h2><p>Suppose we have the following set-up:</p><ul><li>A set of observations $ \pmb{x} = x_1, &mldr;, x_n $</li><li>A set of latent variables $\pmb{z} = z_1, &mldr;, z_l$</li><li>The joint probability distribution is given by $p(x, z)$</li><li>In many probabilistic models we are interested in the posterior distribution $p(\pmb{z} , | , \pmb{x})$ of the latent variables given the observed data $\pmb{x}$:</li></ul><p>$$ p(\pmb{z} , | , \pmb{x}) = \frac{p(\pmb{z},\pmb{x})}{p(\pmb{x})}$$</p><p>This posterior distribution can be used for several purposes. For example, it can be used to provide point estimates for the latent variables.</p><p><strong>Problem:</strong> For many models it&rsquo;s impossible to evaluate this posterior distribution or even to compute expected values with respect to the distribution. To evaluate $p(\pmb{z} , | , \pmb{x})$ we need to compute the denominator $p(\pmb{x})$ which is called the <em>evidence</em>. The evidence is given by $p(\pmb{x}) = \int p(\pmb{z, x}) d\pmb{z}$. For many models this integral cannot be computed or takes exponential time to compute. For example, the dimensionality of the latent variables might be too high.</p><p><strong>Solution:</strong> We approximate $p(\pmb{z} ,|, \pmb{x})$ using variational inference.</p><h2 id=how-does-variational-inference-work>How does variational inference work?</h2><p>To approximate $p(\pmb{z} ,|, \pmb{x})$ we introduce a <em>variational distribution</em> over the latent variables $q(\pmb{z})$. More precisely, we choose a <em>family of distributions</em> $\mathcal{Q}$ characterized by some parameters $\theta$. For example, we could decide that our variational distribution belongs to the family of Gaussian distributions. In this case the parameters $\theta$ would be the mean and standard deviation of the Gaussian distribution. Each member $q(\pmb{z}) \in \mathcal{Q}$ represents a candidate approximation to the true posterior $p(\pmb{z} ,|, \pmb{x})$.</p><p>Our goal is to find the best candidate distribution. Or, to be more precise, to find the setting of the parameters $\theta$ that make our candidate $q(\pmb{z})$ as similar as possible to $p(\pmb{z} ,|, \pmb{x})$.</p><p>Of course the choice of the variational family $\mathcal{Q}$ has a large impact on the final result. The true posterior is often not contained in the variational family. However, we don&rsquo;t need to find the exact posterior. We just want to find a (very) good estimate.</p><h2 id=kl-divergence>KL divergence</h2><p>We evaluate our candidate variational distribution using the <em>Kullback-Leibler divergence</em> which was introduced in <a href>this post</a>. The KL divergence can be used to measure how similar $q(\pmb{z})$ is to the target distribution $p(\pmb{z} ,|, \pmb{x})$. To find the best variational distribution we minimize the KL divergence:</p><p>$$q_{\text{best}}(\pmb{z}) = \arg\min_{q(\pmb{z}) \in \mathcal{Q}} D_{KL}\big(q(\pmb{z}) ,||, p(\pmb{z} ,|, \pmb{x})\big)$$</p><p>The KL divergence is defined as:</p><p>$$D_{KL}\big(q(\pmb{z}) ,||, p(\pmb{z} ,|, \pmb{x})\big) = \int_{-\infty}^{\infty} q(\pmb{z}) \log\big( \frac{q(\pmb{z})}{p(\pmb{z} ,|, \pmb{x})} \big) = \mathbb{E}_{q(\pmb{z})}\big[\log\big( \frac{q(\pmb{z})}{p(\pmb{z} ,|, \pmb{x})} \big) \big]$$</p><p><strong>Problem:</strong> The KL divergence can&rsquo;t be computed. To see why we reformulate the definition of the KL divergence:</p><p>$$ \begin{align}
\mathbb{E}_{q(\pmb{z})}\big[\log\big( \frac{q(\pmb{z})}{p(\pmb{z} | \pmb{x})} \big) \big]
&= \mathbb{E}_{q(\pmb{z})} \big[ \log \big( q(\pmb{z}) \big) \big]- \mathbb{E}_{q(\pmb{z})} \big[ \log \big(p(\pmb{z} | \pmb{x}) \big) \big] \\
&= \mathbb{E}_{q(\pmb{z})} \big[ \log \big( q(\pmb{z}) \big) \big]- \mathbb{E}_{q(\pmb{z})} \big[ \log \big(p(\pmb{z},\pmb{x}) \big) \big] + \log \big( p(\pmb{x}) \big)
\end{align} $$</p><p>The last term in this equation is exactly the evidence we came across earlier $p(\pmb{x}) = \int p(\pmb{z, x}) d\pmb{z}$ and which can&rsquo;t be computed.</p><p><strong>Solution:</strong> Instead of minimizing the KL divergence, we minimize an alternative quanitity which is equivalent up to an added constant. This is the so called <em>evidence lower bound</em> or <em>ELBO</em>:</p><p>$$\text{ELBO}(q) = - \mathbb{E}_{q(\pmb{z})} \big[ \log \big( q(\pmb{z}) \big) \big] + \mathbb{E}_{q(\pmb{z})} \big[ \log \big(p(\pmb{z},\pmb{x}) \big) \big]$$</p><p>When comparing the ELBO with the KL divergence we can see that the ELBO is simply the negative KL divergence plus our problematic evidence term $p(\pmb{x})$. Maximizing the ELBO is equivalent to minimizing the KL divergence.</p><h2 id=evidence-lower-bound>Evidence lower bound</h2><p>To gain a deeper understanding of what it means to find the optimal candidate ${q(\pmb{z})}$ we can rewrite the evidence lower bound:</p><p>$$
\begin{align}
\text{ELBO}(q) &= - \mathbb{E}_{q(\pmb{z})} \big[ \log \big( q(\pmb{z}) \big) \big]+ \mathbb{E}_{q(\pmb{z})} \big[ \log \big(p(\pmb{z},\pmb{x}) \big) \big] \\
&= - \mathbb{E}_{q(\pmb{z})} \big[ \log \big( q(\pmb{z}) \big) \big] + \mathbb{E}_{q(\pmb{z})} \big[ \log \big(p(\pmb{z}) \big) \big] + \mathbb{E}_{q(\pmb{z})} \big[ \log \big(p(\pmb{x} | \pmb{z}) \big) \big] \\
&= \mathbb{E}_{q(\pmb{z})} \big[ \log \big(p(\pmb{x} | \pmb{z}) \big) \big] - D_{KL}\big(q(\pmb{z}) || p(\pmb{z})\big)
\end{align}
$$</p><p>Let&rsquo;s take a closer look at the individual terms:</p><ol><li>The first term $ \mathbb{E}_{q(\pmb{z})} \big[ \log \big(p(\pmb{x} | \pmb{z}) \big) \big]$ describes the probability of the data given the latent variables. By maximizing the ELBO, we encourage the optimization process to choose a candidate distribution $q(\pmb{z})$ which explain the observed data well.</li><li>The second term $- D_{KL}\big(q(\pmb{z}) || p(\pmb{z})\big)$ is the negative KL divergence between our variational distribution $q(\pmb{z})$ and the prior distribution over the latent variables $p(\pmb{z})$. Maximizing this term corresponds to minimizing the KL divergence. So the optimization process is encouraged to make the variational distribution similar to the prior distribution over the latent variables.</li></ol><h2 id=why-its-called-evidence-lower-bound>Why it&rsquo;s called evidence lower bound</h2><p>The name &rsquo;evidence lower bound&rsquo; comes from an important property of the ELBO: it provides a lower bound on the (log) evidence $p(\pmb{x})$.</p><p>We already determined that:</p><ol><li><p>$D_{KL}\big(q(\pmb{z}) || p(\pmb{z} | \pmb{x})\big) = \mathbb{E}_{q(\pmb{z})} \big[ \log \big( q(\pmb{z}) \big) \big] + \mathbb{E}_{q(\pmb{z})} \big[ \log \big(p(\pmb{z},\pmb{x}) \big) \big] + \log \big( p(\pmb{x}) \big) $</p></li><li><p>$\text{ELBO}(q) = - \mathbb{E}_{q(\pmb{z})} \big[ \log \big( q(\pmb{z}) \big) \big] + \mathbb{E}_{q(\pmb{z})} \big[ \log \big(p(\pmb{z},\pmb{x}) \big) \big]$</p></li></ol><p>Combining 1 and 2 gives us:</p><p>$$ D_{KL}\big(q(\pmb{z}) || p(\pmb{z} | \pmb{x})\big) = \log \big( p(\pmb{x}) \big) - \text{ELBO}(q) \ \Leftrightarrow \log \big( p(\pmb{x}) \big) = D_{KL}\big(q(\pmb{z}) || p(\pmb{z} | \pmb{x})\big) + \text{ELBO}(q)$$</p><p>Because the KL divergence is always non-negative, i.e. $D_{KL}(\cdot) \ge 0$ we know that</p><p>$$log \big( p(\pmb{x}) \big) \ge \text{ELBO}(q)$$</p><p>Hence, the ELBO provides a lower bound on the (log) evidence $p(\pmb{x})$.</p><h2 id=example-variational-family>Example variational family</h2><p>The choice of the variational family $\mathcal{Q}$, or rather its complexity determines how complex it will be to optimize the ELBO. In simple terms: if the variational family is very complex, it will be more difficult to solve our optimization problem. One way of restricting the variational family $\mathcal{Q}$ is to choose a parametric distribution $q(\pmb{z} , | , \theta)$ which is goverend by a set of parameters $\theta$. For example, we could choose a Gaussian distribution.</p><p>Another popular approach is the so called <em>mean field approximation</em>. This approach assumes that the variational distribution <em>factorizes</em> with respect to some partition of the latent variables. Mean field approximation works as follows:</p><ol><li>We partition the latent variables $\pmb{z}$ into $M$ disjoint groups $\pmb{z}_i$ with $i = 1, &mldr;, M$.</li><li>We then assume that the variational distribution factorizes with respect to these groups:
$$q(\pmb{z}) = \prod_{i=1}^{M} q_i(\pmb{z}_i)$$</li></ol><p>We don&rsquo;t make any further assumptions about the form of the different $q_i$. They might all be Gaussian distributions or a combination of different distributions. This offers a lot of flexibility. The goal of variational inference is now to find the distribution $q(\pmb{z})$ of the form $q(\pmb{z}) = \prod_{i=1}^{M} q_i(\pmb{z}_i)$ which maximizes the ELBO. To be more precise, we need to optimize the ELBO with respect to all distributions $q_i(\pmb{z}_i)$. This is done by optimizing with respect to each of the factors in turn. More details on this procedure can be found in Bishop&rsquo;s Pattern Recognition and Machine Learning book in chapter 10.1 (a link to the book is given below).</p><h2 id=sources-and-further-reading>Sources and further reading</h2><ul><li>Book: <a href=https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book target=_blank rel=noopener>Pattern Recognition and Machine Learning</a></li><li>Paper: <a href=https://arxiv.org/pdf/1601.00670.pdf target=_blank rel=noopener>Variational Inference: A Review for Statisticians</a></li></ul></div><div class="row ps-3 pe-3"><div class="col-md-6 share-buttons"></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/zotroneneis/zotroneneis.github.io/edit/main/content/posts/machine_learning/variational_inference.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
Improve this page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/machine_learning/support_vector_machines/ title="Support vector machines" class="btn filled-button"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>Support vector machines</div></a></div><div class="col-md-6 next-article"><a href=/posts/python/coding_with_kids/ title="When and how to start coding with kids" class="btn filled-button"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>When and how to start coding with kids</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn type=button data-bs-toggle=tooltip data-bs-placement=left title="Scroll to top"><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center ps-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#what-are-latent-variables>What are latent variables?</a></li><li><a href=#what-is-variational-inference>What is variational inference?</a></li><li><a href=#problem-set-up>Problem set-up</a></li><li><a href=#how-does-variational-inference-work>How does variational inference work?</a></li><li><a href=#kl-divergence>KL divergence</a></li><li><a href=#evidence-lower-bound>Evidence lower bound</a></li><li><a href=#why-its-called-evidence-lower-bound>Why it&rsquo;s called evidence lower bound</a></li><li><a href=#example-variational-family>Example variational family</a></li><li><a href=#sources-and-further-reading>Sources and further reading</a></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-start"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://alpopkes.com/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://alpopkes.com/#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=https://alpopkes.com/#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=https://alpopkes.com/#education>Education</a></li><li class=nav-item><a class=smooth-scroll href=https://alpopkes.com/#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=https://alpopkes.com/#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=https://alpopkes.com/#appearances>Talks & Podcasts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:popkes@gmx.net target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>popkes@gmx.net</span></a></li><li><a href=https://github.com/zotroneneis target=_blank rel=noopener><span><i class="fab fa-github"></i></span> <span>zotroneneis</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-start"><div class=col-md-4><a id=theme href=https://github.com/hugo-toha/toha target=_blank rel=noopener><img src=/images/theme-logo_hu16779671404603505019.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020-2021 Copyright.</div><div class="col-md-4 text-end"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/application.d7aab8398181e71632b7543a04f74db83c8a6210962005e5d34884b8613a8404.js integrity="sha256-16q4OYGB5xYyt1Q6BPdNuDyKYhCWIAXl00iEuGE6hAQ=" defer></script></body></html>