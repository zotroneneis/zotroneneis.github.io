<!doctype html><html><head><title>Variational Inference</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/navigators/navbar.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/images/author/image_al_hu6aa485cfd64a98f54b411e3a0324b396_178285_42x0_resize_q75_box.jpg><link rel=stylesheet href=/css/style.css><meta name=description content="Variational Inference"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/images/author/image_al_hu6aa485cfd64a98f54b411e3a0324b396_178285_42x0_resize_q75_box.jpg>Anna-Lena Popkes</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/images/author/image_al_hu6aa485cfd64a98f54b411e3a0324b396_178285_42x0_resize_q75_box.jpg class=d-none id=main-logo>
<img src=/images/author/image_al_hu6aa485cfd64a98f54b411e3a0324b396_178285_42x0_resize_q75_box.jpg class=d-none id=inverted-logo></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><input type=text placeholder=Search data-search id=search-box><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><a href=/posts/my_path_to_ml/>My path to machine learning</a></li><li><i class="fas fa-plus-circle"></i><a href=/posts/books/>Books</a><ul><li><a href=/posts/books/reading_list/>Personal reading List</a></li><li><a href=/posts/books/deep_work/>Deep work</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/machine_learning/>Machine Learning</a><ul class=active><li><a href=/posts/machine_learning/bayesian_linear_regression/>Bayesian linear regression</a></li><li><a href=/posts/machine_learning/kl_divergence/>KL Divergence</a></li><li><a href=/posts/machine_learning/support_vector_machines/>Support vector machines</a></li><li><a class=active href=/posts/machine_learning/variational_inference/>Variational Inference</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/python/>Python</a><ul><li><a href=/posts/python/mocking/>Mocking</a></li><li><i class="fas fa-plus-circle"></i><a href=/posts/python/magical_universe/>Magical Universe</a><ul><li><a href=/posts/python/magical_universe/day_1_start/>Start</a></li><li><a href=/posts/python/magical_universe/day_1_magical_universe/>The Tales of Castle Kilmere</a></li><li><a href=/posts/python/magical_universe/day_1_first_post_oop/>Object-oriented programming</a></li><li><a href=/posts/python/magical_universe/day_2_types_of_methods/>Types of methods</a></li><li><a href=/posts/python/magical_universe/day_3_type_annotations/>Type annotations</a></li><li><a href=/posts/python/magical_universe/day_4_to_string_conversion/>To-string conversion</a></li><li><a href=/posts/python/magical_universe/day_5_decorators/>Decorators</a></li><li><a href=/posts/python/magical_universe/day_6_properties/>Properties</a></li><li><a href=/posts/python/magical_universe/day_7_underscore_patterns/>Underscore patterns</a></li><li><a href=/posts/python/magical_universe/day_8_extending_universe/>Extending the universe</a></li><li><a href=/posts/python/magical_universe/day_9_duck_typing/>Duck Typing</a></li><li><a href=/posts/python/magical_universe/day_10_11_namedtuples/>Namedtuples</a></li><li><a href=/posts/python/magical_universe/day_12_to_15_abcs/>Abstract Base Classes</a></li><li><a href=/posts/python/magical_universe/day_16_to_18_data_classes/>Data classes</a></li><li><a href=/posts/python/magical_universe/day_19_immutable_data_classes/>Immutable data classes</a></li><li><a href=/posts/python/magical_universe/day_20_decorators_in_classes/>Decorators in classes</a></li><li><a href=/posts/python/magical_universe/day_21_if_main/>if __name__ == "__main__"</a></li><li><a href=/posts/python/magical_universe/day_22_to_24_context_managers/>Context managers</a></li><li><a href=/posts/python/magical_universe/day_25_to_28_pytest/>Testing with pytest</a></li><li><a href=/posts/python/magical_universe/day_29_to_31_iterators/>Iterators</a></li><li><a href=/posts/python/magical_universe/day_34_multisets/>Multisets</a></li><li><a href=/posts/python/magical_universe/day_37_extending_universe/>Extending the universe II</a></li><li><a href=/posts/python/magical_universe/day_43_to_45_exception_classes/>Exception classes</a></li><li><a href=/posts/python/magical_universe/day_46_functools_wraps/>functools.wraps</a></li><li><a href=/posts/python/magical_universe/day_47_to_48_defaultdict/>Defaultdict</a></li><li><a href=/posts/python/magical_universe/day_49_to_50_config_files/>Config files</a></li><li><a href=/posts/python/magical_universe/2018-09-16-blog-post-day-51/>Wrap up</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/software_engineering/>Software Engineering</a><ul><li><a href=/posts/software_engineering/containers/>Intro to containers</a></li><li><a href=/posts/software_engineering/docker/>Intro to Docker</a></li><li><a href=/posts/software_engineering/virtual_machines/>Intro to virtual machines</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://alpopkes.com/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/image_al.jpg><h5 class=author-name>Anna-Lena Popkes</h5><p>February 23, 2019</p></div><div class=title><h1>Variational Inference</h1></div><div class=post-content id=post-content><h1 id=introduction>Introduction</h1><p>Variational inference is an important topic that is widely used in machine learning. For example, it&rsquo;s the basis for variational autoencoders. Also Bayesian learning often makes use variational of inference. To understand what variational inference is, how it works and why it&rsquo;s useful we will go through each point step by step.</p><h2 id=what-are-latent-variables>What are latent variables?</h2><p>A latent variable is the opposite of an <em>observed</em> variable. This means that a latent variable is not directly observed but inferred from other variables which are observed. <a href=https://learnche.org/pid/contents>This book</a> provides a nice conceptual example:</p><blockquote><p><strong>Example:</strong>
Consider your overall health. There are multiple measurements we can use to assess health, were each measurement is looking at a certain physical property, for example blood pressure or body temperature. However, &lsquo;health&rsquo; remains an abstract concept that cannot be measured directly. In this sense, health is a latent variable, whereas blood pressure and body temperature are observable variables.</p></blockquote><h2 id=what-is-variational-inference>What is variational inference?</h2><p>Variational inference is a machine learning method which allows us to approximate probability distributions. In many real world problems we are faced with probability distributions that can&rsquo;t be computed. This especially often happens when a distribution involves latent variables. Therefore, we need strategies to approximate such distributions. Variational inference is one method for doing this. Several other methods exist which broadly fall into two classes: methods that rely on <em>stochastic</em> approximations (like <a href=https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo>Markov chain Monte Carlo sampling</a>) and those that rely on deterministic approximations (like variational inference).</p><p>A key characteristic of variational inference is that it <em>reframes</em> the original problem (computing some probability distribution) into a simpler problem which can be solved. Different to this, Markov chain Monte Carlo sampling directly approximates the target distribution by sampling from it.</p><p>A high level example for variational inference is given <a href=https://www.quora.com/What-is-variational-inference>here</a>:</p><blockquote><p><strong>Example:</strong> Variational inference is similar to what often happens when attending a presentation or lecture. Someone in the audience asks the presenter a very difficult answer which she can&rsquo;t answer. Instead of answering the original, difficult question, the presenter reframes the question into an easier one which can be answered exactly.</p></blockquote><h2 id=problem-set-up>Problem set-up</h2><p>Suppose we have the following set-up:</p><ul><li>A set of observations $ \pmb{x} = x_1, &mldr;, x_n $</li><li>A set of latent variables $\pmb{z} = z_1, &mldr;, z_l$</li><li>The joint probability distribution is given by $p(x, z)$</li><li>In many probabilistic models we are interested in the posterior distribution $p(\pmb{z} , | , \pmb{x})$ of the latent variables given the observed data $\pmb{x}$:</li></ul><p>$$ p(\pmb{z} , | , \pmb{x}) = \frac{p(\pmb{z},\pmb{x})}{p(\pmb{x})}$$</p><p>This posterior distribution can be used for several purposes. For example, it can be used to provide point estimates for the latent variables.</p><p><strong>Problem:</strong> For many models it&rsquo;s impossible to evaluate this posterior distribution or even to compute expected values with respect to the distribution. To evaluate $p(\pmb{z} , | , \pmb{x})$ we need to compute the denominator $p(\pmb{x})$ which is called the <em>evidence</em>. The evidence is given by $p(\pmb{x}) = \int p(\pmb{z, x}) d\pmb{z}$. For many models this integral cannot be computed or takes exponential time to compute. For example, the dimensionality of the latent variables might be too high.</p><p><strong>Solution:</strong> We approximate $p(\pmb{z} ,|, \pmb{x})$ using variational inference.</p><h2 id=how-does-variational-inference-work>How does variational inference work?</h2><p>To approximate $p(\pmb{z} ,|, \pmb{x})$ we introduce a <em>variational distribution</em> over the latent variables $q(\pmb{z})$. More precisely, we choose a <em>family of distributions</em> $\mathcal{Q}$ characterized by some parameters $\theta$. For example, we could decide that our variational distribution belongs to the family of Gaussian distributions. In this case the parameters $\theta$ would be the mean and standard deviation of the Gaussian distribution. Each member $q(\pmb{z}) \in \mathcal{Q}$ represents a candidate approximation to the true posterior $p(\pmb{z} ,|, \pmb{x})$.</p><p>Our goal is to find the best candidate distribution. Or, to be more precise, to find the setting of the parameters $\theta$ that make our candidate $q(\pmb{z})$ as similar as possible to $p(\pmb{z} ,|, \pmb{x})$.</p><p>Of course the choice of the variational family $\mathcal{Q}$ has a large impact on the final result. The true posterior is often not contained in the variational family. However, we don&rsquo;t need to find the exact posterior. We just want to find a (very) good estimate.</p><h2 id=kl-divergence>KL divergence</h2><p>We evaluate our candidate variational distribution using the <em>Kullback-Leibler divergence</em> which was introduced in <a href>this post</a>. The KL divergence can be used to measure how similar $q(\pmb{z})$ is to the target distribution $p(\pmb{z} ,|, \pmb{x})$. To find the best variational distribution we minimize the KL divergence:</p><p>$$q_{\text{best}}(\pmb{z}) = \arg\min_{q(\pmb{z}) \in \mathcal{Q}} D_{KL}\big(q(\pmb{z}) ,||, p(\pmb{z} ,|, \pmb{x})\big)$$</p><p>The KL divergence is defined as:</p><p>$$D_{KL}\big(q(\pmb{z}) ,||, p(\pmb{z} ,|, \pmb{x})\big) = \int_{-\infty}^{\infty} q(\pmb{z}) \log\big( \frac{q(\pmb{z})}{p(\pmb{z} ,|, \pmb{x})} \big) = \mathbb{E}_{q(\pmb{z})}\big[\log\big( \frac{q(\pmb{z})}{p(\pmb{z} ,|, \pmb{x})} \big) \big]$$</p><p><strong>Problem:</strong> The KL divergence can&rsquo;t be computed. To see why we reformulate the definition of the KL divergence:</p><p>$$ \begin{align}
\mathbb{E}_{q(\pmb{z})}\big[\log\big( \frac{q(\pmb{z})}{p(\pmb{z} | \pmb{x})} \big) \big]
&= \mathbb{E}_{q(\pmb{z})} \big[ \log \big( q(\pmb{z}) \big) \big]- \mathbb{E}_{q(\pmb{z})} \big[ \log \big(p(\pmb{z} | \pmb{x}) \big) \big] \\<br>&= \mathbb{E}_{q(\pmb{z})} \big[ \log \big( q(\pmb{z}) \big) \big]- \mathbb{E}_{q(\pmb{z})} \big[ \log \big(p(\pmb{z},\pmb{x}) \big) \big] + \log \big( p(\pmb{x}) \big)
\end{align} $$</p><p>The last term in this equation is exactly the evidence we came across earlier $p(\pmb{x}) = \int p(\pmb{z, x}) d\pmb{z}$ and which can&rsquo;t be computed.</p><p><strong>Solution:</strong> Instead of minimizing the KL divergence, we minimize an alternative quanitity which is equivalent up to an added constant. This is the so called <em>evidence lower bound</em> or <em>ELBO</em>:</p><p>$$\text{ELBO}(q) = - \mathbb{E}_{q(\pmb{z})} \big[ \log \big( q(\pmb{z}) \big) \big] \mathbb{E}_{q(\pmb{z})} \big[ \log \big(p(\pmb{z},\pmb{x}) \big) \big]$$</p><p>When comparing the ELBO with the KL divergence we can see that the ELBO is simply the negative KL divergence plus our problematic evidence term $p(\pmb{x})$. Maximizing the ELBO is equivalent to minimizing the KL divergence.</p><h2 id=evidence-lower-bound>Evidence lower bound</h2><p>To gain a deeper understanding of what it means to find the optimal candidate ${q(\pmb{z})}$ we can rewrite the evidence lower bound:</p><p>$$
\begin{align}
\text{ELBO}(q) &= - \mathbb{E}_{q(\pmb{z})} \big[ \log \big( q(\pmb{z}) \big) \big]+ \mathbb{E}_{q(\pmb{z})} \big[ \log \big(p(\pmb{z},\pmb{x}) \big) \big] \\<br>&= - \mathbb{E}_{q(\pmb{z})} \big[ \log \big( q(\pmb{z}) \big) \big] + \mathbb{E}_{q(\pmb{z})} \big[ \log \big(p(\pmb{z}) \big) \big] + \mathbb{E}_{q(\pmb{z})} \big[ \log \big(p(\pmb{x} | \pmb{z}) \big) \big] \\<br>&= \mathbb{E}_{q(\pmb{z})} \big[ \log \big(p(\pmb{x} | \pmb{z}) \big) \big] - D_{KL}\big(q(\pmb{z}) || p(\pmb{z})\big)
\end{align}
$$</p><p>Let&rsquo;s take a closer look at the individual terms:</p><ol><li>The first term $ \mathbb{E}_{q(\pmb{z})} \big[ \log \big(p(\pmb{x} | \pmb{z}) \big) \big]$ describes the probability of the data given the latent variables. By maximizing the ELBO, we encourage the optimization process to choose a candidate distribution $q(\pmb{z})$ which explain the observed data well.</li><li>The second term $- D_{KL}\big(q(\pmb{z}) || p(\pmb{z})\big)$ is the negative KL divergence between our variational distribution $q(\pmb{z})$ and the prior distribution over the latent variables $p(\pmb{z})$. Maximizing this term corresponds to minimizing the KL divergence. So the optimization process is encouraged to make the variational distribution similar to the prior distribution over the latent variables.</li></ol><h2 id=why-its-called-evidence-lower-bound>Why it&rsquo;s called evidence lower bound</h2><p>The name &lsquo;evidence lower bound&rsquo; comes from an important property of the ELBO: it provides a lower bound on the (log) evidence $p(\pmb{x})$.</p><p>We already determined that:</p><ol><li><p>$D_{KL}\big(q(\pmb{z}) || p(\pmb{z} | \pmb{x})\big) = \mathbb{E}_{q(\pmb{z})} \big[ \log \big( q(\pmb{z}) \big) \big] \mathbb{E}_{q(\pmb{z})} \big[ \log \big(p(\pmb{z},\pmb{x}) \big) \big] + \log \big( p(\pmb{x}) \big) $</p></li><li><p>$\text{ELBO}(q) = - \mathbb{E}_{q(\pmb{z})} \big[ \log \big( q(\pmb{z}) \big) \big] \mathbb{E}_{q(\pmb{z})} \big[ \log \big(p(\pmb{z},\pmb{x}) \big) \big]$</p></li></ol><p>Combining 1 and 2 gives us:</p><p>$$ D_{KL}\big(q(\pmb{z}) || p(\pmb{z} | \pmb{x})\big) = \log \big( p(\pmb{x}) \big) - \text{ELBO}(q) \ \Leftrightarrow \log \big( p(\pmb{x}) \big) = D_{KL}\big(q(\pmb{z}) || p(\pmb{z} | \pmb{x})\big) + \text{ELBO}(q)$$</p><p>Because the KL divergence is always non-negative, i.e. $D_{KL}(\cdot) \ge 0$ we know that</p><p>$$log \big( p(\pmb{x}) \big) \ge \text{ELBO}(q)$$</p><p>Hence, the ELBO provides a lower bound on the (log) evidence $p(\pmb{x})$.</p><h2 id=example-variational-family>Example variational family</h2><p>The choice of the variational family $\mathcal{Q}$, or rather its complexity determines how complex it will be to optimize the ELBO. In simple terms: if the variational family is very complex, it will be more difficult to solve our optimization problem. One way of restricting the variational family $\mathcal{Q}$ is to choose a parametric distribution $q(\pmb{z} , | , \theta)$ which is goverend by a set of parameters $\theta$. For example, we could choose a Gaussian distribution.</p><p>Another popular approach is the so called <em>mean field approximation</em>. This approach assumes that the variational distribution <em>factorizes</em> with respect to some partition of the latent variables. Mean field approximation works as follows:</p><ol><li>We partition the latent variables $\pmb{z}$ into $M$ disjoint groups $\pmb{z}_i$ with $i = 1, &mldr;, M$.</li><li>We then assume that the variational distribution factorizes with respect to these groups:
$$q(\pmb{z}) = \prod_{i=1}^{M} q_i(\pmb{z}_i)$$</li></ol><p>We don&rsquo;t make any further assumptions about the form of the different $q_i$. They might all be Gaussian distributions or a combination of different distributions. This offers a lot of flexibility. The goal of variational inference is now to find the distribution $q(\pmb{z})$ of the form $q(\pmb{z}) = \prod_{i=1}^{M} q_i(\pmb{z}_i)$ which maximizes the ELBO. To be more precise, we need to optimize the ELBO with respect to all distributions $q_i(\pmb{z}_i)$. This is done by optimizing with respect to each of the factors in turn. More details on this procedure can be found in Bishop&rsquo;s Pattern Recognition and Machine Learning book in chapter 10.1 (a link to the book is given below).</p><h2 id=sources-and-further-reading>Sources and further reading</h2><ul><li>Book: <a href=https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book>Pattern Recognition and Machine Learning</a></li><li>Paper: <a href=https://arxiv.org/pdf/1601.00670.pdf>Variational Inference: A Review for Statisticians</a></li></ul></div><div class=btn-improve-page><a href=https://github.com/zotroneneis/zotroneneis.github.io/edit/main/content/posts/machine_learning/variational_inference.md><i class="fas fa-code-branch"></i>
Improve this page</a></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/python/mocking/ class="btn btn-outline-info"><span><i class="fas fa-chevron-circle-left"></i> Prev</span><br><span>Mocking in Python</span></a></div><div class="col-md-6 next-article"><a href=/posts/machine_learning/kl_divergence/ class="btn btn-outline-info"><span>Next <i class="fas fa-chevron-circle-right"></i></span><br><span>Kullback-Leibler Divergence</span></a></div></div><hr></div></div></div></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#what-are-latent-variables>What are latent variables?</a></li><li><a href=#what-is-variational-inference>What is variational inference?</a></li><li><a href=#problem-set-up>Problem set-up</a></li><li><a href=#how-does-variational-inference-work>How does variational inference work?</a></li><li><a href=#kl-divergence>KL divergence</a></li><li><a href=#evidence-lower-bound>Evidence lower bound</a></li><li><a href=#why-its-called-evidence-lower-bound>Why it&rsquo;s called evidence lower bound</a></li><li><a href=#example-variational-family>Example variational family</a></li><li><a href=#sources-and-further-reading>Sources and further reading</a></li></ul></nav></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=#about>About</a></li><li class=nav-item><a class=smooth-scroll href=#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=#education>Education</a></li><li class=nav-item><a class=smooth-scroll href=#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=#appearances>Talks & Podcasts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span>Email:</span> <span>popkes@gmx.net</span></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=#><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png>
Toha</a></div><div class="col-md-4 text-center">Â© 2020-2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/js/jquery-3.4.1.min.js></script><script src=/js/popper.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/navbar.js></script><script src=/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],processEnvironments:!0}}</script></body></html>